{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd009cf3d5909f3828bf53c46564821f879a9c8405e40e27fb09b54d745264e4a6d",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "09cf3d5909f3828bf53c46564821f879a9c8405e40e27fb09b54d745264e4a6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General import of torch.\n",
    "import torch\n",
    "# Import for graph blocks of torch.\n",
    "import torch.nn as nn\n",
    "# Import the models library, to get the model to be used.\n",
    "import torchvision.models as models\n",
    "# Import optim library, to get the optimizer to be used.\n",
    "import torch.optim as optim\n",
    "# Import torchvision, to manage the input data.\n",
    "import torchvision\n",
    "# To apply transformations to the data (when loaded).\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# General imports.\n",
    "import os\n",
    "import time\n",
    "import wandb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "source": [
    "### Enviroment configuration."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input size of the model.\n",
    "inputSize = 'inputSize'\n",
    "# Output size of the model.\n",
    "outputSize = 'outputSize'\n",
    "# Batch size.\n",
    "batchSize = 'batchSize'\n",
    "# Epochs amount.\n",
    "epochs = 'epochs'\n",
    "# Learning rate.\n",
    "learningRate = 'learningRate'\n",
    "# Examples to be showed when requested.\n",
    "testView = 'testView'\n",
    "# Class names.\n",
    "classes = 'classes'\n",
    "# Number of classes to classify.\n",
    "classesLen = 'classesLen'\n",
    "\n",
    "config = {\n",
    "    inputSize    : 224,\n",
    "    outputSize   : 4,\n",
    "    batchSize    : 100,\n",
    "    epochs       : 3,\n",
    "    learningRate : 0.001,\n",
    "    testView     : 8,\n",
    "    classes      : ['COVID', 'Lung Opacity', 'Normal', 'Viral Pneumonia'],\n",
    "    classesLen   : 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device to be used, prefer cuda, if available.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Get the model.\n",
    "# Get a predefined model from pytorch, and get the pretrained parameters.\n",
    "net = models.resnet34(pretrained=True)\n",
    "# Get the input size of the las layer of the model.\n",
    "llInputSize = net.fc.in_features\n",
    "# Modify the last layer of the model, to classify the amount of required classes.\n",
    "net.fc = nn.Linear(llInputSize, config[outputSize])\n",
    "# Load the model to the selected device.\n",
    "net.to(device)\n",
    "\n",
    "# Get criterion and optimizer\n",
    "# Optimizer and the loss funtion used to train the model.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adagrad(net.parameters(), lr=config[learningRate])\n",
    "\n",
    "# Paths of data.\n",
    "#dataPath = 'data/pp/bf'\n",
    "dataPath = 'data/pp/raw'\n",
    "\n",
    "# Run name (for wanbd)\n",
    "runName = 'firstCompleteTest'"
   ]
  },
  {
   "source": [
    "### Miscellaneous functions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This functions process an metrics result dictionary for wandb. Is necessary to indicte\n",
    "#   the metrics origin, training or testing.\n",
    "def processMetricsWandb(metricsResults, training=False):\n",
    "    # Get the prefix to log on wandb, the keys must be different.\n",
    "    resultsType = 'training' if training else 'testing'\n",
    "\n",
    "    # All the wandb keys are based in the original metrics results keys.\n",
    "    lossKey = '{} ({})'.format(_loss, resultsType)\n",
    "    accuracyKey = '{} ({})'.format(_accuracy, resultsType)\n",
    "    accuracyClassKeys = ['{} accuracy ({})'.format(_class, resultsType) for _class in config[classes]]\n",
    "\n",
    "    # Make the dictionary for wandb and store the values.\n",
    "    wandbDict = {\n",
    "        lossKey     : metricsResults[_loss].item(),\n",
    "        accuracyKey : metricsResults[_accuracy].item()\n",
    "    }\n",
    "    for i in range(config[classesLen]):\n",
    "        wandbDict[accuracyClassKeys[i]] = metricsResults[_accuracyClass][i].item()\n",
    "\n",
    "    # Return, to log later.\n",
    "    return wandbDict\n",
    "\n",
    "# Get the metrics dictionaries for wandb and log them.\n",
    "def logMetricsWandb(trainMetricsResults, testMetricsResults):\n",
    "    # Get both dictionaries for wandb.\n",
    "    wandbTrainDict = processMetricsWandb(trainMetricsResults, training=True)\n",
    "    wandbTestDict  = processMetricsWandb(testMetricsResults, training=False)\n",
    "\n",
    "    # Merge the dictionaries.\n",
    "    wandbDict = {**wandbTrainDict, **wandbTestDict}\n",
    "\n",
    "    # Log on wandb\n",
    "    wandb.log(wandbDict)\n",
    "\n",
    "# Pretty print the metrics dictionaries.\n",
    "def printMetricsDict(metricsResults):\n",
    "    # Build accuracy by class.\n",
    "    accuracyClassStr = ''\n",
    "    for i, _class in enumerate(config[classes]):\n",
    "        accuracyClassStr += '{}: {:2.2f}%'.format(_class, metricsResults[_accuracyClass][i] * 100)\n",
    "        accuracyClassStr += ', '\n",
    "    accuracyClassStr = accuracyClassStr[:-2]\n",
    "\n",
    "    print('Loss: {:.4f}, Accuracy: {:2.2f}% ({})'.format(metricsResults[_loss], metricsResults[_accuracy] * 100, accuracyClassStr))\n",
    "\n",
    "# About metrics.\n",
    "# Metric dictionary keys \n",
    "_loss            = 'Loss'\n",
    "_accuracy        = 'Accuracy'\n",
    "_accuracyClass   = 'Accuracy class'\n",
    "_confusionMatrix = 'Confusion matrix'\n",
    "\n",
    "# Get a clean dictionary for the metrics.\n",
    "def getMetricsDict():\n",
    "    return {\n",
    "        _loss            : torch.tensor(0.),\n",
    "        _accuracy        : torch.tensor(0.),\n",
    "        _accuracyClass   : torch.zeros(config[classesLen]),\n",
    "        _confusionMatrix : torch.zeros((config[classesLen], config[classesLen]), dtype=torch.int)\n",
    "    }\n",
    "\n",
    "# Function used to update the dictionary of resulting metrics.\n",
    "def updateRunningMetrics(outputs, groundtruth, loss, batchAmount, metricsResults):\n",
    "    # Accumulate the loss.\n",
    "    metricsResults[_loss] += loss.cpu() / batchAmount\n",
    "    # Accumulate the confusion matrix.\n",
    "    confusionMatrix = getConfusionMatrix(outputs, groundtruth)\n",
    "    metricsResults[_confusionMatrix] += confusionMatrix\n",
    "\n",
    "# Function used to process the dictionary of resulting metrics (make final calculations).\n",
    "def processRunningMetrics(metricsResults):\n",
    "    # Get the total of samples processed by class.\n",
    "    classTotal = torch.sum(metricsResults[_confusionMatrix], 1)\n",
    "    # Get the total of samples correctly classified by class.\n",
    "    classCorrect = torch.diagonal(metricsResults[_confusionMatrix])\n",
    "\n",
    "    # Get the total accuracy, correct total samples / total samples.\n",
    "    metricsResults[_accuracy] = torch.sum(classCorrect) / torch.sum(classTotal)\n",
    "    # Get the total accuracy, by class.\n",
    "    metricsResults[_accuracyClass] = classCorrect / classTotal"
   ]
  },
  {
   "source": [
    "### Loader function.\n",
    "Should return the training loader and test loader, a iterable object by batches."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to get the data loaders.\n",
    "# A folder with two folders inside called train and test is expected as a rootPath.\n",
    "def getLoaders(rootPath):\n",
    "    # Transformation definitions.\n",
    "    transformTrain = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(config[inputSize]),  # This one does a resize (it cuts randomly, it doesn't keep the whole image).\n",
    "            transforms.RandomHorizontalFlip(),                # Flip the image horizontally randomly.\n",
    "            transforms.ToTensor(),                            # Make the image a tensor.\n",
    "            transforms.Normalize(\n",
    "                [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Expected normalization for pretrained pytorch models.\n",
    "        ])\n",
    "    transformTest = transforms.Compose([\n",
    "            transforms.Resize(config[inputSize]),             # Resize the image, keeping all pixels.\n",
    "            transforms.CenterCrop(config[inputSize]),         # Cut the image in the center.\n",
    "            transforms.ToTensor(),                            # Make the image a tensor.\n",
    "            transforms.Normalize(\n",
    "                [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Expected normalization for pretrained pytorch models.\n",
    "        ])\n",
    "\n",
    "    trainPath = os.path.join(rootPath, 'train')\n",
    "    testPath  = os.path.join(rootPath, 'test')\n",
    "\n",
    "    # Get the training and test data, apply the transformations.\n",
    "    trainset = torchvision.datasets.ImageFolder(root=trainPath, transform=transformTrain)\n",
    "    testset  = torchvision.datasets.ImageFolder(root=testPath,  transform=transformTest)\n",
    "\n",
    "    # Get the loaders, to iterate the data through batches.\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=config[batchSize], shuffle=True, num_workers=2)\n",
    "    testloader  = torch.utils.data.DataLoader(testset,  batch_size=config[batchSize], shuffle=True, num_workers=2)\n",
    "\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "source": [
    "### Calculate the confusion matrix."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the confusion matrix values.\n",
    "def getConfusionMatrix(outputs, groundtruth):\n",
    "    # Init the confusion matrix.\n",
    "    confusionMatrix = torch.zeros((config[classesLen], config[classesLen]), dtype=torch.int)\n",
    "\n",
    "    # Obtain the predictions (the greater number, because we use a one hot vector).\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # Iterate the predictions.\n",
    "    for i in range(predicted.shape[0]):\n",
    "        # Add 1 based on the prediction done for a specific label.\n",
    "        confusionMatrix[groundtruth[i]][predicted[i]] += 1\n",
    "\n",
    "    return confusionMatrix"
   ]
  },
  {
   "source": [
    "### Training method.\n",
    "This method takes care of a single training pass. Another function call this one multiple times."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEpoch(dataloader, model, criterion, optimizer):\n",
    "    # Metrics for training.\n",
    "    metricsResults = getMetricsDict()\n",
    "\n",
    "    # Enable the grad, for training.\n",
    "    with torch.set_grad_enabled(True):\n",
    "\n",
    "        # Indicate that the model is going to be trained.\n",
    "        model.train()\n",
    "\n",
    "        # Loader len, for metrics calculation.\n",
    "        loaderLen = len(dataloader)\n",
    "\n",
    "        # Iterate the batches for training.\n",
    "        for batch in dataloader:\n",
    "            # Train the model.\n",
    "            # Get the inputs and labels, and move them to the selected device.\n",
    "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "            # Zero the gradient parameters.\n",
    "            optimizer.zero_grad()\n",
    "            # Get the predictions.\n",
    "            outputs = model(inputs)\n",
    "            # Calculate the error.\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Calculates the derivatives of the parameters that have a gradient.\n",
    "            loss.backward()\n",
    "            # Update the parameters based on the computer gradient.\n",
    "            optimizer.step()\n",
    "            # Metrics for the training set.\n",
    "            updateRunningMetrics(outputs, labels, loss, loaderLen, metricsResults)\n",
    "\n",
    "    return metricsResults"
   ]
  },
  {
   "source": [
    "### Evaluation method.\n",
    "This method evaluates the model for a specified dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, criterion):\n",
    "    # Metrics for testing.\n",
    "    metricsResults = getMetricsDict()\n",
    "\n",
    "    # Enable the grad, for training.\n",
    "    with torch.set_grad_enabled(False):\n",
    "\n",
    "        # Indicate that the model is going to be evaluated.\n",
    "        model.eval()\n",
    "\n",
    "        # Loader len, for metrics calculation.\n",
    "        loaderLen = len(dataloader)\n",
    "\n",
    "        # Iterate the batches for testing.\n",
    "        for batch in dataloader:\n",
    "            # Test the model.\n",
    "            # Get the inputs and labels, and move them to the selected device.\n",
    "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "            # Get the predictions.\n",
    "            outputs = model(inputs)\n",
    "            # Calculate the error.\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Metrics for the testing set.\n",
    "            updateRunningMetrics(outputs, labels, loss, loaderLen, metricsResults)\n",
    "\n",
    "    return metricsResults"
   ]
  },
  {
   "source": [
    "### Trainining and evaluate method.\n",
    "For the specific purpose of this project, in each epoch we evaluate metrics for each data set (training and testing) in each epoch, this method simplifies the process. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndEvaluate(trainloader, testloader, model, criterion, optimizer):\n",
    "\n",
    "    startTimeTotal = time.time()\n",
    "\n",
    "    for epoch in range(1, config[epochs] + 1):\n",
    "        \n",
    "        # Train the model.\n",
    "        trainMetricsResults = trainEpoch(trainloader, model, criterion, optimizer)\n",
    "        processRunningMetrics(trainMetricsResults)\n",
    "\n",
    "        # Evaluate the model.\n",
    "        testMetricsResults = evaluate(testloader, model, criterion)\n",
    "        processRunningMetrics(testMetricsResults)\n",
    "\n",
    "        # Log on wandb\n",
    "        logMetricsWandb(trainMetricsResults, testMetricsResults)\n",
    "\n",
    "        # Print the results.\n",
    "        print('**', '[', 'Epoch ', epoch, ']', '*' * 48, sep='')\n",
    "        print('\\tTraining results:', end=' ')\n",
    "        printMetricsDict(trainMetricsResults)\n",
    "        print('\\t Testing results:', end=' ')\n",
    "        printMetricsDict(testMetricsResults)\n",
    "        \n",
    "    # Print time\n",
    "    print('Epochs terminados')\n",
    "    print(\"--- %s seconds ---\" % (time.time() - startTimeTotal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpablobrenes\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "/home/pablo/miniconda3/lib/python3.8/site-packages/IPython/html.py:12: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  warn(\"The `IPython.html` package has been deprecated since IPython 4.0. \"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.29<br/>\n                Syncing run <strong style=\"color:#cdcd00\">firstCompleteTest</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/tecai/CNN\" target=\"_blank\">https://wandb.ai/tecai/CNN</a><br/>\n                Run page: <a href=\"https://wandb.ai/tecai/CNN/runs/1rfdbihr\" target=\"_blank\">https://wandb.ai/tecai/CNN/runs/1rfdbihr</a><br/>\n                Run data is saved locally in <code>/home/pablo/Desktop/ML/Proyectos/Proyecto II/wandb/run-20210508_193229-1rfdbihr</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "**[Epoch 1]************************************************\n",
      "\tTraining results: Loss: 0.4536, Accuracy: 83.40% (COVID: 74.18%, Lung Opacity: 79.69%, Normal: 89.55%, Viral Pneumonia: 78.20%)\n",
      "\t Testing results: Loss: 0.2864, Accuracy: 88.97% (COVID: 78.95%, Lung Opacity: 93.78%, Normal: 92.21%, Viral Pneumonia: 68.99%)\n",
      "**[Epoch 2]************************************************\n",
      "\tTraining results: Loss: 0.2821, Accuracy: 89.75% (COVID: 86.78%, Lung Opacity: 85.16%, Normal: 93.72%, Viral Pneumonia: 88.22%)\n",
      "\t Testing results: Loss: 0.3750, Accuracy: 86.27% (COVID: 63.41%, Lung Opacity: 78.94%, Normal: 99.36%, Viral Pneumonia: 81.40%)\n",
      "Run pip install nbformat to save notebook history\n",
      "**[Epoch 3]************************************************\n",
      "\tTraining results: Loss: 0.2389, Accuracy: 91.41% (COVID: 89.89%, Lung Opacity: 87.25%, Normal: 94.53%, Viral Pneumonia: 90.43%)\n",
      "\t Testing results: Loss: 0.3990, Accuracy: 85.38% (COVID: 75.65%, Lung Opacity: 71.89%, Normal: 99.85%, Viral Pneumonia: 61.24%)\n",
      "Epochs terminados\n",
      "--- 154.91491961479187 seconds ---\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 22628<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/home/pablo/Desktop/ML/Proyectos/Proyecto II/wandb/run-20210508_193229-1rfdbihr/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/home/pablo/Desktop/ML/Proyectos/Proyecto II/wandb/run-20210508_193229-1rfdbihr/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>Loss (training)</td><td>0.2389</td></tr><tr><td>Accuracy (training)</td><td>0.91407</td></tr><tr><td>COVID accuracy (training)</td><td>0.89893</td></tr><tr><td>Lung Opacity accuracy (training)</td><td>0.87245</td></tr><tr><td>Normal accuracy (training)</td><td>0.94528</td></tr><tr><td>Viral Pneumonia accuracy (training)</td><td>0.90432</td></tr><tr><td>Loss (testing)</td><td>0.39895</td></tr><tr><td>Accuracy (testing)</td><td>0.85377</td></tr><tr><td>COVID accuracy (testing)</td><td>0.75653</td></tr><tr><td>Lung Opacity accuracy (testing)</td><td>0.71891</td></tr><tr><td>Normal accuracy (testing)</td><td>0.99853</td></tr><tr><td>Viral Pneumonia accuracy (testing)</td><td>0.6124</td></tr><tr><td>_runtime</td><td>157</td></tr><tr><td>_timestamp</td><td>1620524106</td></tr><tr><td>_step</td><td>2</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>Loss (training)</td><td>█▂▁</td></tr><tr><td>Accuracy (training)</td><td>▁▇█</td></tr><tr><td>COVID accuracy (training)</td><td>▁▇█</td></tr><tr><td>Lung Opacity accuracy (training)</td><td>▁▆█</td></tr><tr><td>Normal accuracy (training)</td><td>▁▇█</td></tr><tr><td>Viral Pneumonia accuracy (training)</td><td>▁▇█</td></tr><tr><td>Loss (testing)</td><td>▁▇█</td></tr><tr><td>Accuracy (testing)</td><td>█▃▁</td></tr><tr><td>COVID accuracy (testing)</td><td>█▁▇</td></tr><tr><td>Lung Opacity accuracy (testing)</td><td>█▃▁</td></tr><tr><td>Normal accuracy (testing)</td><td>▁██</td></tr><tr><td>Viral Pneumonia accuracy (testing)</td><td>▄█▁</td></tr><tr><td>_runtime</td><td>▁▄█</td></tr><tr><td>_timestamp</td><td>▁▄█</td></tr><tr><td>_step</td><td>▁▅█</td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">firstCompleteTest</strong>: <a href=\"https://wandb.ai/tecai/CNN/runs/1rfdbihr\" target=\"_blank\">https://wandb.ai/tecai/CNN/runs/1rfdbihr</a><br/>\n                "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Get the loaders.\n",
    "trainloader, testloader = getLoaders(dataPath)\n",
    "\n",
    "# Init wandb\n",
    "run = wandb.init(project='CNN', entity='tecai', config=config, name=runName)\n",
    "#wandb.watch(net)\n",
    "\n",
    "# Train and evaluate\n",
    "trainAndEvaluate(trainloader, testloader, net, criterion, optimizer)\n",
    "\n",
    "# Finish wandb\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo.\n",
    "#PATH = './cifar_net.pth'\n",
    "#torch.save(net.state_dict(), PATH)\n",
    "\n",
    "# Cargar el modelo.\n",
    "# Esta linea de abajo debe coincidir con el modelo que voy a usar.\n",
    "#   instancio el mismo modelo que guardé\n",
    "# net = Net()\n",
    "# net.load_state_dict(torch.load(PATH))"
   ]
  }
 ]
}