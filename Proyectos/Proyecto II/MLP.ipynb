{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python395jvsc74a57bd03a31fab793c547cb35fe033c429e016f6dd189ef070db975037acd85972e51d4",
   "display_name": "Python 3.9.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "3a31fab793c547cb35fe033c429e016f6dd189ef070db975037acd85972e51d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile\n",
    "from os.path import isdir\n",
    "from os.path import join\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import shutil\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "import wandb"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 17,
   "outputs": []
  },
  {
   "source": [
    "Misc functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "  X = []\n",
    "  Y = []\n",
    "\n",
    "  onlyDirs = [f for f in listdir(path) if isdir(join(path, f))]\n",
    "\n",
    "  for category in onlyDirs:\n",
    "    pos_y = onlyDirs.index(category)\n",
    "    print(category, pos_y)\n",
    "    tempDirInput = join(path, category)\n",
    "    for f in listdir(tempDirInput):\n",
    "      inputPath = (join(tempDirInput, f))\n",
    "      if isfile(inputPath):\n",
    "        X.append(np.load(inputPath))\n",
    "        one_hot = np.zeros((len(onlyDirs)))\n",
    "        one_hot[pos_y] = 1\n",
    "        Y.append(one_hot)\n",
    "\n",
    "  return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y):\n",
    "    \n",
    "    top_pred = y_pred.argmax(1, keepdim = True)\n",
    "    \n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# About metrics.\n",
    "# Metric dictionary keys \n",
    "_loss            = 'Loss'\n",
    "_accuracy        = 'Accuracy'\n",
    "_accuracyClass   = 'Accuracy class'\n",
    "_confusionMatrix = 'Confusion matrix'\n",
    "_groundtruth     = 'Groundtruth'\n",
    "_predictions     = 'Predictions'\n",
    "# Get a clean dictionary for the metrics.\n",
    "def getMetricsDict(config):\n",
    "    return {\n",
    "        _loss            : torch.tensor(0.),\n",
    "        _accuracy        : torch.tensor(0.),\n",
    "        _accuracyClass   : torch.zeros(config[classesLen]),\n",
    "        _confusionMatrix : torch.zeros((config[classesLen], config[classesLen]), dtype=torch.int),\n",
    "        _groundtruth     : torch.tensor([]),\n",
    "        _predictions     : torch.tensor([])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty print the metrics dictionaries.\n",
    "def printMetricsDict(metricsResults, config):\n",
    "    # Build accuracy by class.\n",
    "    accuracyClassStr = ''\n",
    "    for i, _class in enumerate(config[classes]):\n",
    "        accuracyClassStr += '{}: {:2.2f}%'.format(_class, metricsResults[_accuracyClass][i] * 100)\n",
    "        accuracyClassStr += ', '\n",
    "    accuracyClassStr = accuracyClassStr[:-2]\n",
    "\n",
    "    print('Loss: {:.4f}, Accuracy: {:2.2f}% ({})'.format(metricsResults[_loss], metricsResults[_accuracy] * 100, accuracyClassStr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to update the dictionary of resulting metrics.\n",
    "def updateRunningMetrics(outputs, groundtruth, loss, batchAmount, metricsResults, config):\n",
    "    # Accumulate the loss.\n",
    "    metricsResults[_loss] += loss.cpu() / batchAmount\n",
    "    # Accumulate the confusion matrix.\n",
    "    confusionMatrix = getConfusionMatrix(outputs, groundtruth, config)\n",
    "    metricsResults[_confusionMatrix] += confusionMatrix\n",
    "    metricsResults[_groundtruth] = torch.cat((metricsResults[_groundtruth], groundtruth)) \n",
    "    metricsResults[_predictions] = torch.cat((metricsResults[_predictions], outputs))\n",
    "\n",
    "# Function used to process the dictionary of resulting metrics (make final calculations).\n",
    "def processRunningMetrics(metricsResults):\n",
    "    # Get the total of samples processed by class.\n",
    "    classTotal = torch.sum(metricsResults[_confusionMatrix], 1)\n",
    "    # Get the total of samples correctly classified by class.\n",
    "    classCorrect = torch.diagonal(metricsResults[_confusionMatrix])\n",
    "\n",
    "    # Get the total accuracy, correct total samples / total samples.\n",
    "    metricsResults[_accuracy] = torch.sum(classCorrect) / torch.sum(classTotal)\n",
    "    # Get the total accuracy, by class.\n",
    "    metricsResults[_accuracyClass] = classCorrect / classTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This functions process an metrics result dictionary for wandb. Is necessary to indicte\n",
    "#   the metrics origin, training or testing.\n",
    "def processMetricsWandb(metricsResults, config, training=False):\n",
    "    # Get the prefix to log on wandb, the keys must be different.\n",
    "    resultsType = 'training' if training else 'testing'\n",
    "\n",
    "    # All the wandb keys are based in the original metrics results keys.\n",
    "    lossKey = '{} ({})'.format(_loss, resultsType)\n",
    "    accuracyKey = '{} ({})'.format(_accuracy, resultsType)\n",
    "    accuracyClassKeys = ['{} accuracy ({})'.format(_class, resultsType) for _class in config[classes]]\n",
    "\n",
    "    confusionMatrixKey = '{} ({})'.format(_confusionMatrix, resultsType)\n",
    "    heatMap = wandb.sklearn.plot_confusion_matrix(torch.argmax(metricsResults[_groundtruth], axis=1), torch.argmax(metricsResults[_predictions], axis=1), config[classes])\n",
    "\n",
    "    # Make the dictionary for wandb and store the values.\n",
    "    wandbDict = {\n",
    "        lossKey             : metricsResults[_loss].item(),\n",
    "        accuracyKey         : metricsResults[_accuracy].item(),\n",
    "        confusionMatrixKey  : heatMap\n",
    "    }\n",
    "    for i in range(config[classesLen]):\n",
    "        wandbDict[accuracyClassKeys[i]] = metricsResults[_accuracyClass][i].item()\n",
    "\n",
    "    # Return, to log later.\n",
    "    return wandbDict\n",
    "    \n",
    "# Get the metrics dictionaries for wandb and log them.\n",
    "def logMetricsWandb(trainMetricsResults, testMetricsResults):\n",
    "    # Get both dictionaries for wandb.\n",
    "    wandbTrainDict = processMetricsWandb(trainMetricsResults, training=True)\n",
    "    wandbTestDict  = processMetricsWandb(testMetricsResults, training=False)\n",
    "\n",
    "    # Merge the dictionaries.\n",
    "    wandbDict = {**wandbTrainDict, **wandbTestDict}\n",
    "\n",
    "    # Log on wandb\n",
    "    wandb.log(wandbDict)"
   ]
  },
  {
   "source": [
    "# Calculate the confusion matrix.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the confusion matrix values.\n",
    "def getConfusionMatrix(outputs, groundtruth, config):\n",
    "    # Init the confusion matrix.\n",
    "    confusionMatrix = torch.zeros((config[classesLen], config[classesLen]), dtype=torch.int)\n",
    "\n",
    "    # Obtain the predictions (the greater number, because we use a one hot vector).\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # Iterate the predictions.\n",
    "    for i in range(predicted.shape[0]):\n",
    "        # Add 1 based on the prediction done for a specific label.\n",
    "        \n",
    "        confusionMatrix[torch.argmax(groundtruth[i])][predicted[i]] += 1\n",
    "\n",
    "    return confusionMatrix"
   ]
  },
  {
   "source": [
    "Paths dir"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder paths\n",
    "histOutputDir = 'data/pp/hist'\n",
    "\n",
    "run_ID = 0"
   ]
  },
  {
   "source": [
    "# Load training and testing set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "COVID 0\n",
      "Lung_Opacity 1\n",
      "Normal 2\n",
      "Viral_Pneumonia 3\n",
      "COVID 0\n",
      "Lung_Opacity 1\n",
      "Normal 2\n",
      "Viral_Pneumonia 3\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = load_data(os.path.join(histOutputDir,'test'))\n",
    "X_train, y_train = load_data(os.path.join(histOutputDir,'train'))"
   ]
  },
  {
   "source": [
    "Global parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input size of the model.\n",
    "inputSize = 'inputSize'\n",
    "# Output size of the model.\n",
    "outputSize = 'outputSize'\n",
    "# Batch size.\n",
    "batchSize = 'batchSize'\n",
    "# Epochs amount.\n",
    "epochs = 'epochs'\n",
    "# Learning rate.\n",
    "learningRate = 'learningRate'\n",
    "#FC_Layer\n",
    "fc_size_1 = 'fc_size_1'\n",
    "fc_size_2 = 'fc_size_2'\n",
    "fc_size_3 = 'fc_size_3'\n",
    "fc_size_4 = 'fc_size_4'\n",
    "fc_size_5 = 'fc_size_5'\n",
    "fc_size_6 = 'fc_size_6'\n",
    "# Class names.\n",
    "classes = 'classes'\n",
    "# Number of classes to classify.\n",
    "classesLen = 'classesLen'\n",
    "name = 'name'\n",
    "\n",
    "upSampling = 'upSampling'\n",
    "\n",
    "exp_1 = {\n",
    "    epochs        :   21,\n",
    "    learningRate :   0.0001,\n",
    "    batchSize    :   32,\n",
    "    inputSize     :   256,\n",
    "    fc_size_1     :   256,\n",
    "    fc_size_2     :   128,\n",
    "    fc_size_3     :   64,\n",
    "    fc_size_4     :   32,\n",
    "    fc_size_5     :   16,\n",
    "    fc_size_6     :   8,\n",
    "    outputSize      :   4,\n",
    "    classes      : ['COVID', 'Lung Opacity', 'Normal', 'Viral Pneumonia'],\n",
    "    classesLen     : 4,\n",
    "    upSampling     : False,\n",
    "    name           : 'WithoutUpsampling_32Batch'\n",
    "}\n",
    "exp_2 = {\n",
    "    epochs        :   21,\n",
    "    learningRate :   0.0001,\n",
    "    batchSize    :   32,\n",
    "    inputSize     :   256,\n",
    "    fc_size_1     :   256,\n",
    "    fc_size_2     :   128,\n",
    "    fc_size_3     :   64,\n",
    "    fc_size_4     :   32,\n",
    "    fc_size_5     :   16,\n",
    "    fc_size_6     :   8,\n",
    "    outputSize      :   4,\n",
    "    classes      : ['COVID', 'Lung Opacity', 'Normal', 'Viral Pneumonia'],\n",
    "    classesLen     : 4,\n",
    "    upSampling     : True,\n",
    "    name           : 'WithUpsampling_32Batch'\n",
    "}\n",
    "exp_3 = {\n",
    "    epochs        :   21,\n",
    "    learningRate :   0.0001,\n",
    "    batchSize    :   16,\n",
    "    inputSize     :   256,\n",
    "    fc_size_1     :   256,\n",
    "    fc_size_2     :   128,\n",
    "    fc_size_3     :   64,\n",
    "    fc_size_4     :   32,\n",
    "    fc_size_5     :   16,\n",
    "    fc_size_6     :   8,\n",
    "    outputSize      :   4,\n",
    "    classes      : ['COVID', 'Lung Opacity', 'Normal', 'Viral Pneumonia'],\n",
    "    classesLen     : 4,\n",
    "    upSampling     : False,\n",
    "    name           : 'WithoutUpsampling_16Batch'\n",
    "}\n",
    "exp_4 = {\n",
    "    epochs        :   21,\n",
    "    learningRate :   0.0001,\n",
    "    batchSize    :   16,\n",
    "    inputSize     :   256,\n",
    "    fc_size_1     :   256,\n",
    "    fc_size_2     :   128,\n",
    "    fc_size_3     :   64,\n",
    "    fc_size_4     :   32,\n",
    "    fc_size_5     :   16,\n",
    "    fc_size_6     :   8,\n",
    "    outputSize      :   4,\n",
    "    classes      : ['COVID', 'Lung Opacity', 'Normal', 'Viral Pneumonia'],\n",
    "    classesLen     : 4,\n",
    "    upSampling     : True,\n",
    "    name           : 'WithUpsampling_16Batch'\n",
    "}\n",
    "\n",
    "configs = [exp_1, exp_2, exp_3, exp_4]"
   ]
  },
  {
   "source": [
    "## Create a Data Loader"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### UpSampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upSamplingData(config):\n",
    "    if config[upSampling]:\n",
    "        dic_values = {}\n",
    "        weight = {}\n",
    "        for class_type in np.unique(y_train, axis=0):\n",
    "            dic_values[int(np.argmax(class_type))] = len(np.where((y_train == class_type).all(axis=1))[0])\n",
    "            weight[int(np.argmax(class_type))] = float(1.0/len(np.unique(y_train, axis=0)))/dic_values[int (np.argmax(class_type))]\n",
    "\n",
    "        print(\"dic_values\", dic_values)\n",
    "        print(\"weight\", weight)\n",
    "\n",
    "        samples_weight = np.array([weight[int(np.argmax(t))] for t in y_train])\n",
    "        samples_weight = torch.from_numpy(samples_weight)\n",
    "        sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "\n",
    "        return sampler\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampler option is mutually exclusive with shuffle option.\n",
    "def setShuffle(config):\n",
    "    if config[upSampling]:\n",
    "        return False\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataLoaders(config):\n",
    "        tensor_x = torch.Tensor(X_train) \n",
    "        tensor_y = torch.Tensor(y_train)\n",
    "        my_dataset = TensorDataset(tensor_x,tensor_y) \n",
    "        trainingSetLoader = DataLoader(my_dataset,\n",
    "                shuffle=setShuffle(config),\n",
    "                sampler= upSamplingData(config),\n",
    "                batch_size=config[batchSize]) \n",
    "\n",
    "        tensor_x_test = torch.Tensor(X_test) \n",
    "        tensor_y_test = torch.Tensor(y_test)\n",
    "        my_dataset_test = TensorDataset(tensor_x_test,tensor_y_test) \n",
    "        testSetLoader = DataLoader(my_dataset_test,\n",
    "                shuffle=True,\n",
    "                batch_size=config[batchSize]) \n",
    "        return (trainingSetLoader, testSetLoader)\n"
   ]
  },
  {
   "source": [
    "Check CUDA is available"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "source": [
    "## Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, config):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, config[fc_size_1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config[fc_size_1], config[fc_size_2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config[fc_size_2], config[fc_size_3]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config[fc_size_3], config[fc_size_4]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(config[fc_size_4], config[fc_size_5]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config[fc_size_5], config[fc_size_6]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config[fc_size_6], output_dim),\n",
    "            nn.Softmax()\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    "
   ]
  },
  {
   "source": [
    "### Training method.\n",
    "This method takes care of a single training pass. Another function call this one multiple times."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEpoch(dataloader, model, criterion, optimizer, config):\n",
    "    # Metrics for training.\n",
    "    metricsResults = getMetricsDict(config)\n",
    "\n",
    "    # Enable the grad, for training.\n",
    "    with torch.set_grad_enabled(True):\n",
    "\n",
    "        # Indicate that the model is going to be trained.\n",
    "        model.train()\n",
    "\n",
    "        # Loader len, for metrics calculation.\n",
    "        loaderLen = len(dataloader)\n",
    "\n",
    "        # Iterate the batches for training.\n",
    "        for batch in dataloader:\n",
    "            # Train the model.\n",
    "            # Get the inputs and labels, and move them to the selected device.\n",
    "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "            # Zero the gradient parameters.\n",
    "            optimizer.zero_grad()\n",
    "            # Get the predictions.\n",
    "            outputs = model(inputs)\n",
    "            # Calculate the error.\n",
    "            labels_argmax = torch.empty(labels.shape[0], dtype=torch.long)\n",
    "            labels_argmax = torch.argmax(labels, dim=1)\n",
    "            loss = criterion(outputs, labels_argmax)\n",
    "            # Calculates the derivatives of the parameters that have a gradient.\n",
    "            loss.backward()\n",
    "            # Update the parameters based on the computer gradient.\n",
    "            optimizer.step()\n",
    "            # Metrics for the training set.\n",
    "            updateRunningMetrics(outputs, labels, loss, loaderLen, metricsResults, config)\n",
    "\n",
    "    return metricsResults"
   ]
  },
  {
   "source": [
    "### Evaluation method.\n",
    "This method evaluates the model for a specified dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, criterion, config):\n",
    "    # Metrics for testing.\n",
    "    metricsResults = getMetricsDict(config)\n",
    "\n",
    "    # Enable the grad, for training.\n",
    "    with torch.set_grad_enabled(False):\n",
    "\n",
    "        # Indicate that the model is going to be evaluated.\n",
    "        model.eval()\n",
    "\n",
    "        # Loader len, for metrics calculation.\n",
    "        loaderLen = len(dataloader)\n",
    "\n",
    "        # Iterate the batches for testing.\n",
    "        for batch in dataloader:\n",
    "            # Test the model.\n",
    "            # Get the inputs and labels, and move them to the selected device.\n",
    "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "            # Get the predictions.\n",
    "            outputs = model(inputs)\n",
    "            # Calculate the error.\n",
    "            labels_argmax = torch.empty(labels.shape[0], dtype=torch.long)\n",
    "            labels_argmax = torch.argmax(labels, dim=1)\n",
    "            loss = criterion(outputs, labels_argmax)\n",
    "            # Metrics for the testing set.\n",
    "            updateRunningMetrics(outputs, labels, loss, loaderLen, metricsResults, config)\n",
    "\n",
    "    return metricsResults"
   ]
  },
  {
   "source": [
    "### Trainining and evaluate method.\n",
    "For the specific purpose of this project, in each epoch we evaluate metrics for each data set (training and testing) in each epoch, this method simplifies the process. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndEvaluate(trainloader, testloader, model, criterion, optimizer, config):\n",
    "    startTimeTotal = time.time()\n",
    "    print('Running training')\n",
    "    for epoch in range(1, config[epochs] + 1):\n",
    "        \n",
    "        # Train the model.\n",
    "        trainMetricsResults = trainEpoch(trainloader, model, criterion, optimizer, config)\n",
    "        processRunningMetrics(trainMetricsResults)\n",
    "\n",
    "        # Evaluate the model.\n",
    "        testMetricsResults = evaluate(testloader, model, criterion, config)\n",
    "        processRunningMetrics(testMetricsResults)\n",
    "\n",
    "        # Log on wandb\n",
    "        logMetricsWandb(trainMetricsResults, testMetricsResults)\n",
    "\n",
    "        # Print the results.\n",
    "        if epoch %20 == 0:\n",
    "            print('**', '[', 'Epoch ', epoch, ']', '*' * 48, sep='')\n",
    "            print('\\tTraining results:', end=' ')\n",
    "            printMetricsDict(trainMetricsResults)\n",
    "            print('\\t Testing results:', end=' ')\n",
    "            printMetricsDict(testMetricsResults)\n",
    "        \n",
    "    # Print time\n",
    "    print('Epochs terminados')\n",
    "    print(\"--- %s seconds ---\" % (time.time() - startTimeTotal))"
   ]
  },
  {
   "source": [
    "# Run"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\TEC/.netrc\n",
      "C:\\Users\\TEC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\html.py:12: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  warn(\"The `IPython.html` package has been deprecated since IPython 4.0. \"\n",
      "wandb: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.29<br/>\n                Syncing run <strong style=\"color:#cdcd00\">WithoutUpsampling_32Batch6</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/tecai/MLP\" target=\"_blank\">https://wandb.ai/tecai/MLP</a><br/>\n                Run page: <a href=\"https://wandb.ai/tecai/MLP/runs/lnhgse0k\" target=\"_blank\">https://wandb.ai/tecai/MLP/runs/lnhgse0k</a><br/>\n                Run data is saved locally in <code>c:\\Users\\TEC\\Documents\\GitHub\\ML\\Proyectos\\Proyecto II\\wandb\\run-20210510_135732-lnhgse0k</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running training\n",
      "C:\\Users\\TEC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py:119: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "processRunningMetrics() takes 1 positional argument but 2 were given",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-d5a0e103aaba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mwandb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m#Train and evaluate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mtrainAndEvaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunConfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;31m#Finish WandB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-2aa8ec2719f1>\u001b[0m in \u001b[0;36mtrainAndEvaluate\u001b[1;34m(trainloader, testloader, model, criterion, optimizer, config)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;31m# Train the model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mtrainMetricsResults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainEpoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mprocessRunningMetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainMetricsResults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# Evaluate the model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: processRunningMetrics() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "#Create the model, optimizer and criterion\n",
    "run_ID += 1\n",
    "for runConfig in configs:\n",
    "\n",
    "    trainloader, testloader = createDataLoaders(runConfig)\n",
    "    \n",
    "    model = MLP(runConfig[inputSize], runConfig[outputSize], runConfig)\n",
    "    optimizer = optim.Adam(model.parameters(),lr=runConfig[learningRate])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    #Init WandB\n",
    "    name = runConfig[name] + str(run_ID)\n",
    "    run = wandb.init(project='MLP', entity='tecai', config=runConfig,\n",
    "                            name=name)\n",
    "    wandb.watch(model)\n",
    "    #Train and evaluate the model\n",
    "    trainAndEvaluate(trainloader, testloader, model, criterion, optimizer, runConfig)\n",
    "    #Finish WandB\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}