{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python390jvsc74a57bd063fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d",
   "display_name": "Python 3.9.0 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile\n",
    "from os.path import isdir\n",
    "from os.path import join\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import shutil\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import wandb"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": []
  },
  {
   "source": [
    "Misc functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "  X = []\n",
    "  Y = []\n",
    "\n",
    "  onlyDirs = [f for f in listdir(path) if isdir(join(path, f))]\n",
    "\n",
    "  for category in onlyDirs:\n",
    "    pos_y = onlyDirs.index(category)\n",
    "    print(category, pos_y)\n",
    "    tempDirInput = join(path, category)\n",
    "    for f in listdir(tempDirInput):\n",
    "      inputPath = (join(tempDirInput, f))\n",
    "      if isfile(inputPath):\n",
    "        X.append(np.load(inputPath))\n",
    "        one_hot = np.zeros((len(onlyDirs)))\n",
    "        one_hot[pos_y] = 1\n",
    "        Y.append(one_hot)\n",
    "\n",
    "  return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y):\n",
    "    \n",
    "    top_pred = y_pred.argmax(1, keepdim = True)\n",
    "    \n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "source": [
    "Paths dir"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder paths\n",
    "histOutputDir = 'data/pp/hist'\n",
    "\n",
    "run_ID = 2"
   ]
  },
  {
   "source": [
    "# Load training and testing set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "COVID 0\n",
      "Lung_Opacity 1\n",
      "Normal 2\n",
      "Viral Pneumonia 3\n",
      "COVID 0\n",
      "Lung_Opacity 1\n",
      "Normal 2\n",
      "Viral Pneumonia 3\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = load_data(os.path.join(histOutputDir,'test'))\n",
    "X_train, y_train = load_data(os.path.join(histOutputDir,'train'))"
   ]
  },
  {
   "source": [
    "Global parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "configwand = {\n",
    "    'epochs'        :   600,\n",
    "    'learning_rate' :   0.0001,\n",
    "    'batch_size'    :   128,\n",
    "    'input_dim'     :   256,\n",
    "    'fc_size_1'     :   256,\n",
    "    'fc_size_2'     :   128,\n",
    "    'fc_size_3'     :   64,\n",
    "    'fc_size_4'     :   32,\n",
    "    'fc_size_5'     :   16,\n",
    "    'fc_size_6'     :   8,\n",
    "    'output_dim'    :   4\n",
    "}"
   ]
  },
  {
   "source": [
    "## Create a Data Loader"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_x = torch.Tensor(X_train) \n",
    "tensor_y = torch.Tensor(y_train)\n",
    "my_dataset = TensorDataset(tensor_x,tensor_y) \n",
    "trainingSetLoader = DataLoader(my_dataset,\n",
    "        shuffle=True,\n",
    "        batch_size=configwand['batch_size']) \n",
    "\n",
    "tensor_x_test = torch.Tensor(X_test) \n",
    "tensor_y_test = torch.Tensor(y_test)\n",
    "my_dataset_test = TensorDataset(tensor_x_test,tensor_y_test) \n",
    "testSetLoader = DataLoader(my_dataset_test,\n",
    "        shuffle=True,\n",
    "        batch_size=configwand['batch_size']) \n"
   ]
  },
  {
   "source": [
    "## Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, configwand['fc_size_1']),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(configwand['fc_size_1'], configwand['fc_size_2']),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(configwand['fc_size_2'], configwand['fc_size_3']),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(configwand['fc_size_3'], configwand['fc_size_4']),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(configwand['fc_size_4'], configwand['fc_size_5']),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(configwand['fc_size_5'], configwand['fc_size_6']),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(configwand['fc_size_6'], output_dim),\n",
    "            nn.Softmax()\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndEvaluate(model, optimizer, criterion):\n",
    "    for epoch in range(0, configwand['epochs']):\n",
    "        is_training = True\n",
    "        with torch.set_grad_enabled(is_training):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            for i_Train, (x, y) in enumerate(trainingSetLoader):\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(x)\n",
    "                y_argmax = torch.empty(y.shape[0], dtype=torch.long)\n",
    "                y_argmax = torch.argmax(y, dim=1)\n",
    "                loss = criterion(y_pred, y_argmax)\n",
    "                \n",
    "                acc = calculate_accuracy(y_pred, y_argmax)\n",
    "                loss.backward()\n",
    "        \n",
    "                running_loss += loss.item()\n",
    "                running_acc += acc\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        is_training = False\n",
    "\n",
    "        running_test_loss = 0.0\n",
    "        running_test_acc = 0.0\n",
    "\n",
    "        with torch.set_grad_enabled(is_training):\n",
    "            for i_Test, (x, y) in enumerate(testSetLoader):\n",
    "                \n",
    "                y_pred = model(x)\n",
    "                y_argmax = torch.empty(y.shape[0], dtype=torch.long)\n",
    "                y_argmax = torch.argmax(y, dim=1)\n",
    "\n",
    "                loss = criterion(y_pred, y_argmax)\n",
    "                acc = calculate_accuracy(y_pred, y_argmax)\n",
    "        \n",
    "                running_test_loss += loss.item()\n",
    "                running_test_acc += acc\n",
    "\n",
    "        \n",
    "        wandb.log({\"Training Loss\"       :  running_loss/i_Train,\n",
    "                    \"Training Accuracy\"   :  running_acc/i_Train})\n",
    "\n",
    "        wandb.log({\"Test Loss\"       :  running_test_loss/i_Test,\n",
    "                    \"Test Accuracy\"   :  running_test_acc/i_Test})\n",
    "\n",
    "        if(epoch%20 == 0):\n",
    "            print(\"Epoch %d, training loss: {%.3f}, training acc {%.3f}\" % (epoch, \n",
    "                    running_loss/i_Train, running_acc/i_Train))\n",
    "            print(\"Epoch %d, testing loss: {%.3f}, testing acc {%.3f}\" % (epoch, \n",
    "                    running_test_loss/i_Test, running_test_acc/i_Test))\n",
    "            print('******************************************')\n",
    "            \n",
    "model = MLP(configwand['input_dim'], configwand['output_dim'])\n",
    "optimizer = optim.Adam(model.parameters(),lr=configwand['learning_rate'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "trainAndEvaluate(model,optimizer,criterion)"
   ]
  },
  {
   "source": [
    "# Run"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "run_ID += 1\n",
    "print(run_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: stevenpach10 (use `wandb login --relogin` to force relogin)\n",
      "C:\\Users\\Steven\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\html.py:12: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  warn(\"The `IPython.html` package has been deprecated since IPython 4.0. \"\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.29<br/>\n                Syncing run <strong style=\"color:#cdcd00\">MLP 4</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/tecai/MLP\" target=\"_blank\">https://wandb.ai/tecai/MLP</a><br/>\n                Run page: <a href=\"https://wandb.ai/tecai/MLP/runs/2u04gspd\" target=\"_blank\">https://wandb.ai/tecai/MLP/runs/2u04gspd</a><br/>\n                Run data is saved locally in <code>c:\\Users\\Steven\\Documents\\Github\\ML\\Proyectos\\Proyecto II\\wandb\\run-20210506_135644-2u04gspd</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py:119: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n",
      "C:\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:145: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:109.)\n",
      "  Variable._execution_engine.run_backward(\n",
      "Epoch 0, training loss: {1.371}, training acc {0.482}\n",
      "Epoch 0, testing loss: {1.398}, testing acc {0.515}\n",
      "******************************************\n",
      "Epoch 20, training loss: {1.197}, training acc {0.549}\n",
      "Epoch 20, testing loss: {1.210}, testing acc {0.573}\n",
      "******************************************\n",
      "Epoch 40, training loss: {1.162}, training acc {0.595}\n",
      "Epoch 40, testing loss: {1.180}, testing acc {0.613}\n",
      "******************************************\n",
      "Epoch 60, training loss: {1.143}, training acc {0.614}\n",
      "Epoch 60, testing loss: {1.164}, testing acc {0.628}\n",
      "******************************************\n",
      "Epoch 80, training loss: {1.135}, training acc {0.620}\n",
      "Epoch 80, testing loss: {1.155}, testing acc {0.637}\n",
      "******************************************\n",
      "Epoch 100, training loss: {1.126}, training acc {0.629}\n",
      "Epoch 100, testing loss: {1.145}, testing acc {0.647}\n",
      "******************************************\n",
      "Epoch 120, training loss: {1.117}, training acc {0.638}\n",
      "Epoch 120, testing loss: {1.142}, testing acc {0.652}\n",
      "******************************************\n",
      "Epoch 140, training loss: {1.112}, training acc {0.644}\n",
      "Epoch 140, testing loss: {1.141}, testing acc {0.653}\n",
      "******************************************\n",
      "Epoch 160, training loss: {1.106}, training acc {0.650}\n",
      "Epoch 160, testing loss: {1.130}, testing acc {0.663}\n",
      "******************************************\n",
      "Epoch 180, training loss: {1.101}, training acc {0.656}\n",
      "Epoch 180, testing loss: {1.121}, testing acc {0.673}\n",
      "******************************************\n",
      "Epoch 200, training loss: {1.098}, training acc {0.657}\n",
      "Epoch 200, testing loss: {1.130}, testing acc {0.663}\n",
      "******************************************\n",
      "Epoch 220, training loss: {1.095}, training acc {0.659}\n",
      "Epoch 220, testing loss: {1.130}, testing acc {0.664}\n",
      "******************************************\n",
      "Epoch 240, training loss: {1.091}, training acc {0.664}\n",
      "Epoch 240, testing loss: {1.120}, testing acc {0.674}\n",
      "******************************************\n",
      "Epoch 260, training loss: {1.086}, training acc {0.668}\n",
      "Epoch 260, testing loss: {1.120}, testing acc {0.672}\n",
      "******************************************\n",
      "Epoch 280, training loss: {1.082}, training acc {0.674}\n",
      "Epoch 280, testing loss: {1.117}, testing acc {0.675}\n",
      "******************************************\n",
      "Epoch 300, training loss: {1.080}, training acc {0.675}\n",
      "Epoch 300, testing loss: {1.109}, testing acc {0.685}\n",
      "******************************************\n",
      "Epoch 320, training loss: {1.077}, training acc {0.678}\n",
      "Epoch 320, testing loss: {1.110}, testing acc {0.686}\n",
      "******************************************\n",
      "Epoch 340, training loss: {1.075}, training acc {0.682}\n",
      "Epoch 340, testing loss: {1.103}, testing acc {0.691}\n",
      "******************************************\n",
      "Epoch 360, training loss: {1.073}, training acc {0.682}\n",
      "Epoch 360, testing loss: {1.111}, testing acc {0.683}\n",
      "******************************************\n",
      "Epoch 380, training loss: {1.067}, training acc {0.688}\n",
      "Epoch 380, testing loss: {1.112}, testing acc {0.681}\n",
      "******************************************\n",
      "Epoch 400, training loss: {1.068}, training acc {0.688}\n",
      "Epoch 400, testing loss: {1.102}, testing acc {0.693}\n",
      "******************************************\n",
      "Epoch 420, training loss: {1.066}, training acc {0.690}\n",
      "Epoch 420, testing loss: {1.100}, testing acc {0.695}\n",
      "******************************************\n",
      "Epoch 440, training loss: {1.064}, training acc {0.693}\n",
      "Epoch 440, testing loss: {1.101}, testing acc {0.691}\n",
      "******************************************\n",
      "Epoch 460, training loss: {1.063}, training acc {0.694}\n",
      "Epoch 460, testing loss: {1.100}, testing acc {0.695}\n",
      "******************************************\n",
      "Epoch 480, training loss: {1.061}, training acc {0.696}\n",
      "Epoch 480, testing loss: {1.105}, testing acc {0.689}\n",
      "******************************************\n",
      "Epoch 500, training loss: {1.061}, training acc {0.696}\n",
      "Epoch 500, testing loss: {1.096}, testing acc {0.698}\n",
      "******************************************\n",
      "Epoch 520, training loss: {1.059}, training acc {0.697}\n",
      "Epoch 520, testing loss: {1.103}, testing acc {0.691}\n",
      "******************************************\n",
      "Epoch 540, training loss: {1.057}, training acc {0.699}\n",
      "Epoch 540, testing loss: {1.096}, testing acc {0.700}\n",
      "******************************************\n",
      "Epoch 560, training loss: {1.057}, training acc {0.700}\n",
      "Epoch 560, testing loss: {1.093}, testing acc {0.702}\n",
      "******************************************\n",
      "Epoch 580, training loss: {1.056}, training acc {0.701}\n",
      "Epoch 580, testing loss: {1.098}, testing acc {0.697}\n",
      "******************************************\n",
      "Run pip install nbformat to save notebook history\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 1816<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>c:\\Users\\Steven\\Documents\\Github\\ML\\Proyectos\\Proyecto II\\wandb\\run-20210506_135644-2u04gspd\\logs\\debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>c:\\Users\\Steven\\Documents\\Github\\ML\\Proyectos\\Proyecto II\\wandb\\run-20210506_135644-2u04gspd\\logs\\debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>Training Loss</td><td>1.055</td></tr><tr><td>Training Accuracy</td><td>0.70061</td></tr><tr><td>_runtime</td><td>920</td></tr><tr><td>_timestamp</td><td>1620331924</td></tr><tr><td>_step</td><td>1199</td></tr><tr><td>Test Loss</td><td>1.09571</td></tr><tr><td>Test Accuracy</td><td>0.70007</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>Training Loss</td><td>█▇▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training Accuracy</td><td>▁▁▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>Test Loss</td><td>█▇▅▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▁▁▁▁▁▂▁▂▁▁▁▁</td></tr><tr><td>Test Accuracy</td><td>▁▂▄▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇███▇▇█████▇█▇████</td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">MLP 4</strong>: <a href=\"https://wandb.ai/tecai/MLP/runs/2u04gspd\" target=\"_blank\">https://wandb.ai/tecai/MLP/runs/2u04gspd</a><br/>\n                "
     },
     "metadata": {}
    }
   ],
   "source": [
    "#Create the model, optimizer and criterion\n",
    "run_ID += 1\n",
    "model = MLP(configwand['input_dim'], configwand['output_dim'])\n",
    "optimizer = optim.Adam(model.parameters(),lr=configwand['learning_rate'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#Init WandB\n",
    "name = \"MLP \"+ str(run_ID)\n",
    "run = wandb.init(project='MLP', entity='tecai', config=configwand,\n",
    "                        name=name)\n",
    "wandb.watch(model)\n",
    "#Train and evaluate the model\n",
    "trainAndEvaluate(model,optimizer,criterion)\n",
    "#Finish WandB\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}