{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python390jvsc74a57bd063fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d",
   "display_name": "Python 3.9.0 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile\n",
    "from os.path import isdir\n",
    "from os.path import join\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import shutil\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "import wandb"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "source": [
    "Misc functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "  X = []\n",
    "  Y = []\n",
    "\n",
    "  onlyDirs = [f for f in listdir(path) if isdir(join(path, f))]\n",
    "\n",
    "  for category in onlyDirs:\n",
    "    pos_y = onlyDirs.index(category)\n",
    "    print(category, pos_y)\n",
    "    tempDirInput = join(path, category)\n",
    "    for f in listdir(tempDirInput):\n",
    "      inputPath = (join(tempDirInput, f))\n",
    "      if isfile(inputPath):\n",
    "        X.append(np.load(inputPath))\n",
    "        one_hot = np.zeros((len(onlyDirs)))\n",
    "        one_hot[pos_y] = 1\n",
    "        Y.append(one_hot)\n",
    "\n",
    "  return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y):\n",
    "    \n",
    "    top_pred = y_pred.argmax(1, keepdim = True)\n",
    "    \n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# About metrics.\n",
    "# Metric dictionary keys \n",
    "_loss            = 'Loss'\n",
    "_accuracy        = 'Accuracy'\n",
    "_accuracyClass   = 'Accuracy class'\n",
    "_confusionMatrix = 'Confusion matrix'\n",
    "# Get a clean dictionary for the metrics.\n",
    "def getMetricsDict():\n",
    "    return {\n",
    "        _loss            : torch.tensor(0.),\n",
    "        _accuracy        : torch.tensor(0.),\n",
    "        _accuracyClass   : torch.zeros(config[classesLen]),\n",
    "        _confusionMatrix : torch.zeros((config[classesLen], config[classesLen]), dtype=torch.int)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty print the metrics dictionaries.\n",
    "def printMetricsDict(metricsResults):\n",
    "    # Build accuracy by class.\n",
    "    accuracyClassStr = ''\n",
    "    for i, _class in enumerate(config[classes]):\n",
    "        accuracyClassStr += '{}: {:2.2f}%'.format(_class, metricsResults[_accuracyClass][i] * 100)\n",
    "        accuracyClassStr += ', '\n",
    "    accuracyClassStr = accuracyClassStr[:-2]\n",
    "\n",
    "    print('Loss: {:.4f}, Accuracy: {:2.2f}% ({})'.format(metricsResults[_loss], metricsResults[_accuracy] * 100, accuracyClassStr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to update the dictionary of resulting metrics.\n",
    "def updateRunningMetrics(outputs, groundtruth, loss, batchAmount, metricsResults):\n",
    "    # Accumulate the loss.\n",
    "    metricsResults[_loss] += loss.cpu() / batchAmount\n",
    "    # Accumulate the confusion matrix.\n",
    "    confusionMatrix = getConfusionMatrix(outputs, groundtruth)\n",
    "    metricsResults[_confusionMatrix] += confusionMatrix\n",
    "\n",
    "# Function used to process the dictionary of resulting metrics (make final calculations).\n",
    "def processRunningMetrics(metricsResults):\n",
    "    # Get the total of samples processed by class.\n",
    "    classTotal = torch.sum(metricsResults[_confusionMatrix], 1)\n",
    "    # Get the total of samples correctly classified by class.\n",
    "    classCorrect = torch.diagonal(metricsResults[_confusionMatrix])\n",
    "\n",
    "    # Get the total accuracy, correct total samples / total samples.\n",
    "    metricsResults[_accuracy] = torch.sum(classCorrect) / torch.sum(classTotal)\n",
    "    # Get the total accuracy, by class.\n",
    "    metricsResults[_accuracyClass] = classCorrect / classTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This functions process an metrics result dictionary for wandb. Is necessary to indicte\n",
    "#   the metrics origin, training or testing.\n",
    "def processMetricsWandb(metricsResults, training=False):\n",
    "    # Get the prefix to log on wandb, the keys must be different.\n",
    "    resultsType = 'training' if training else 'testing'\n",
    "\n",
    "    # All the wandb keys are based in the original metrics results keys.\n",
    "    lossKey = '{} ({})'.format(_loss, resultsType)\n",
    "    accuracyKey = '{} ({})'.format(_accuracy, resultsType)\n",
    "    accuracyClassKeys = ['{} accuracy ({})'.format(_class, resultsType) for _class in config[classes]]\n",
    "\n",
    "    # Make the dictionary for wandb and store the values.\n",
    "    wandbDict = {\n",
    "        lossKey     : metricsResults[_loss].item(),\n",
    "        accuracyKey : metricsResults[_accuracy].item()\n",
    "    }\n",
    "    for i in range(config[classesLen]):\n",
    "        wandbDict[accuracyClassKeys[i]] = metricsResults[_accuracyClass][i].item()\n",
    "\n",
    "    # Return, to log later.\n",
    "    return wandbDict\n",
    "    \n",
    "# Get the metrics dictionaries for wandb and log them.\n",
    "def logMetricsWandb(trainMetricsResults, testMetricsResults):\n",
    "    # Get both dictionaries for wandb.\n",
    "    wandbTrainDict = processMetricsWandb(trainMetricsResults, training=True)\n",
    "    wandbTestDict  = processMetricsWandb(testMetricsResults, training=False)\n",
    "\n",
    "    # Merge the dictionaries.\n",
    "    wandbDict = {**wandbTrainDict, **wandbTestDict}\n",
    "\n",
    "    # Log on wandb\n",
    "    wandb.log(wandbDict)"
   ]
  },
  {
   "source": [
    "# Calculate the confusion matrix.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the confusion matrix values.\n",
    "def getConfusionMatrix(outputs, groundtruth):\n",
    "    # Init the confusion matrix.\n",
    "    confusionMatrix = torch.zeros((config[classesLen], config[classesLen]), dtype=torch.int)\n",
    "\n",
    "    # Obtain the predictions (the greater number, because we use a one hot vector).\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # Iterate the predictions.\n",
    "    for i in range(predicted.shape[0]):\n",
    "        # Add 1 based on the prediction done for a specific label.\n",
    "        \n",
    "        confusionMatrix[torch.argmax(groundtruth[i])][predicted[i]] += 1\n",
    "\n",
    "    return confusionMatrix"
   ]
  },
  {
   "source": [
    "Paths dir"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder paths\n",
    "histOutputDir = 'data/pp/hist'\n",
    "\n",
    "run_ID = 0"
   ]
  },
  {
   "source": [
    "# Load training and testing set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "COVID 0\n",
      "Lung_Opacity 1\n",
      "Normal 2\n",
      "Viral_Pneumonia 3\n",
      "COVID 0\n",
      "Lung_Opacity 1\n",
      "Normal 2\n",
      "Viral_Pneumonia 3\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = load_data(os.path.join(histOutputDir,'test'))\n",
    "X_train, y_train = load_data(os.path.join(histOutputDir,'train'))"
   ]
  },
  {
   "source": [
    "Global parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input size of the model.\n",
    "inputSize = 'inputSize'\n",
    "# Output size of the model.\n",
    "outputSize = 'outputSize'\n",
    "# Batch size.\n",
    "batchSize = 'batchSize'\n",
    "# Epochs amount.\n",
    "epochs = 'epochs'\n",
    "# Learning rate.\n",
    "learningRate = 'learningRate'\n",
    "#FC_Layer\n",
    "fc_size_1 = 'fc_size_1'\n",
    "fc_size_2 = 'fc_size_2'\n",
    "fc_size_3 = 'fc_size_3'\n",
    "fc_size_4 = 'fc_size_4'\n",
    "fc_size_5 = 'fc_size_5'\n",
    "fc_size_6 = 'fc_size_6'\n",
    "# Class names.\n",
    "classes = 'classes'\n",
    "# Number of classes to classify.\n",
    "classesLen = 'classesLen'\n",
    "\n",
    "upSampling = 'upSampling'\n",
    "\n",
    "config = {\n",
    "    epochs        :   600,\n",
    "    learningRate :   0.0001,\n",
    "    batchSize    :   128,\n",
    "    inputSize     :   256,\n",
    "    fc_size_1     :   256,\n",
    "    fc_size_2     :   128,\n",
    "    fc_size_3     :   64,\n",
    "    fc_size_4     :   32,\n",
    "    fc_size_5     :   16,\n",
    "    fc_size_6     :   8,\n",
    "    outputSize      :   4,\n",
    "    classes      : ['COVID', 'Lung Opacity', 'Normal', 'Viral Pneumonia'],\n",
    "    classesLen     : 4,\n",
    "    upSampling     : True\n",
    "}"
   ]
  },
  {
   "source": [
    "## Create a Data Loader"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### UpSampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upSamplingData():\n",
    "    if config[upSampling]:\n",
    "        dic_values = {}\n",
    "        weight = {}\n",
    "        for class_type in np.unique(y_train, axis=0):\n",
    "            dic_values[int(np.argmax(class_type))] = len(np.where((y_train == class_type).all(axis=1))[0])\n",
    "            weight[int(np.argmax(class_type))] = float(1.0/len(np.unique(y_train, axis=0)))/dic_values[int (np.argmax(class_type))]\n",
    "\n",
    "        print(\"dic_values\", dic_values)\n",
    "        print(\"weight\", weight)\n",
    "\n",
    "        samples_weight = np.array([weight[int(np.argmax(t))] for t in y_train])\n",
    "        samples_weight = torch.from_numpy(samples_weight)\n",
    "        sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "\n",
    "        return sampler\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampler option is mutually exclusive with shuffle option.\n",
    "def setShuffle():\n",
    "    if config[upSampling]:\n",
    "        return False\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataLoaders():\n",
    "        tensor_x = torch.Tensor(X_train) \n",
    "        tensor_y = torch.Tensor(y_train)\n",
    "        my_dataset = TensorDataset(tensor_x,tensor_y) \n",
    "        trainingSetLoader = DataLoader(my_dataset,\n",
    "                shuffle=setShuffle(),\n",
    "                sampler= upSamplingData(),\n",
    "                batch_size=config[batchSize]) \n",
    "\n",
    "        tensor_x_test = torch.Tensor(X_test) \n",
    "        tensor_y_test = torch.Tensor(y_test)\n",
    "        my_dataset_test = TensorDataset(tensor_x_test,tensor_y_test) \n",
    "        testSetLoader = DataLoader(my_dataset_test,\n",
    "                shuffle=True,\n",
    "                batch_size=config[batchSize]) \n",
    "        return (trainingSetLoader, testSetLoader)\n"
   ]
  },
  {
   "source": [
    "Check CUDA is available"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "source": [
    "## Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, config[fc_size_1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config[fc_size_1], config[fc_size_2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config[fc_size_2], config[fc_size_3]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config[fc_size_3], config[fc_size_4]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(config[fc_size_4], config[fc_size_5]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config[fc_size_5], config[fc_size_6]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config[fc_size_6], output_dim),\n",
    "            nn.Softmax()\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    "
   ]
  },
  {
   "source": [
    "### Training method.\n",
    "This method takes care of a single training pass. Another function call this one multiple times."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEpoch(dataloader, model, criterion, optimizer):\n",
    "    # Metrics for training.\n",
    "    metricsResults = getMetricsDict()\n",
    "\n",
    "    # Enable the grad, for training.\n",
    "    with torch.set_grad_enabled(True):\n",
    "\n",
    "        # Indicate that the model is going to be trained.\n",
    "        model.train()\n",
    "\n",
    "        # Loader len, for metrics calculation.\n",
    "        loaderLen = len(dataloader)\n",
    "\n",
    "        # Iterate the batches for training.\n",
    "        for batch in dataloader:\n",
    "            # Train the model.\n",
    "            # Get the inputs and labels, and move them to the selected device.\n",
    "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "            # Zero the gradient parameters.\n",
    "            optimizer.zero_grad()\n",
    "            # Get the predictions.\n",
    "            outputs = model(inputs)\n",
    "            # Calculate the error.\n",
    "            labels_argmax = torch.empty(labels.shape[0], dtype=torch.long)\n",
    "            labels_argmax = torch.argmax(labels, dim=1)\n",
    "            loss = criterion(outputs, labels_argmax)\n",
    "            # Calculates the derivatives of the parameters that have a gradient.\n",
    "            loss.backward()\n",
    "            # Update the parameters based on the computer gradient.\n",
    "            optimizer.step()\n",
    "            # Metrics for the training set.\n",
    "            updateRunningMetrics(outputs, labels, loss, loaderLen, metricsResults)\n",
    "\n",
    "    return metricsResults"
   ]
  },
  {
   "source": [
    "### Evaluation method.\n",
    "This method evaluates the model for a specified dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, criterion):\n",
    "    # Metrics for testing.\n",
    "    metricsResults = getMetricsDict()\n",
    "\n",
    "    # Enable the grad, for training.\n",
    "    with torch.set_grad_enabled(False):\n",
    "\n",
    "        # Indicate that the model is going to be evaluated.\n",
    "        model.eval()\n",
    "\n",
    "        # Loader len, for metrics calculation.\n",
    "        loaderLen = len(dataloader)\n",
    "\n",
    "        # Iterate the batches for testing.\n",
    "        for batch in dataloader:\n",
    "            # Test the model.\n",
    "            # Get the inputs and labels, and move them to the selected device.\n",
    "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "            # Get the predictions.\n",
    "            outputs = model(inputs)\n",
    "            # Calculate the error.\n",
    "            labels_argmax = torch.empty(labels.shape[0], dtype=torch.long)\n",
    "            labels_argmax = torch.argmax(labels, dim=1)\n",
    "            loss = criterion(outputs, labels_argmax)\n",
    "            # Metrics for the testing set.\n",
    "            updateRunningMetrics(outputs, labels, loss, loaderLen, metricsResults)\n",
    "\n",
    "    return metricsResults"
   ]
  },
  {
   "source": [
    "### Trainining and evaluate method.\n",
    "For the specific purpose of this project, in each epoch we evaluate metrics for each data set (training and testing) in each epoch, this method simplifies the process. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndEvaluate(trainloader, testloader, model, criterion, optimizer):\n",
    "    startTimeTotal = time.time()\n",
    "    print('Running training')\n",
    "    for epoch in range(1, config[epochs] + 1):\n",
    "        \n",
    "        # Train the model.\n",
    "        trainMetricsResults = trainEpoch(trainloader, model, criterion, optimizer)\n",
    "        processRunningMetrics(trainMetricsResults)\n",
    "\n",
    "        # Evaluate the model.\n",
    "        testMetricsResults = evaluate(testloader, model, criterion)\n",
    "        processRunningMetrics(testMetricsResults)\n",
    "\n",
    "        # Log on wandb\n",
    "        logMetricsWandb(trainMetricsResults, testMetricsResults)\n",
    "\n",
    "        # Print the results.\n",
    "        if epoch %20 == 0:\n",
    "            print('**', '[', 'Epoch ', epoch, ']', '*' * 48, sep='')\n",
    "            print('\\tTraining results:', end=' ')\n",
    "            printMetricsDict(trainMetricsResults)\n",
    "            print('\\t Testing results:', end=' ')\n",
    "            printMetricsDict(testMetricsResults)\n",
    "        \n",
    "    # Print time\n",
    "    print('Epochs terminados')\n",
    "    print(\"--- %s seconds ---\" % (time.time() - startTimeTotal))"
   ]
  },
  {
   "source": [
    "# Run"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.30<br/>\n                Syncing run <strong style=\"color:#cdcd00\">MLP Steven 9</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/tecai/MLP\" target=\"_blank\">https://wandb.ai/tecai/MLP</a><br/>\n                Run page: <a href=\"https://wandb.ai/tecai/MLP/runs/1oqem14f\" target=\"_blank\">https://wandb.ai/tecai/MLP/runs/1oqem14f</a><br/>\n                Run data is saved locally in <code>c:\\Users\\Steven\\Documents\\Github\\ML\\Proyectos\\Proyecto II\\wandb\\run-20210509_111505-1oqem14f</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "**[Epoch 20]************************************************\n",
      "\tTraining results: Loss: 1.2064, Accuracy: 49.86% (COVID: 17.00%, Lung Opacity: 0.81%, Normal: 97.08%, Viral Pneumonia: 0.00%)\n",
      "\t Testing results: Loss: 1.2028, Accuracy: 50.48% (COVID: 19.12%, Lung Opacity: 2.24%, Normal: 96.52%, Viral Pneumonia: 0.00%)\n",
      "**[Epoch 40]************************************************\n",
      "\tTraining results: Loss: 1.1578, Accuracy: 59.56% (COVID: 33.64%, Lung Opacity: 51.23%, Normal: 81.61%, Viral Pneumonia: 0.00%)\n",
      "\t Testing results: Loss: 1.1441, Accuracy: 59.93% (COVID: 34.80%, Lung Opacity: 52.40%, Normal: 80.90%, Viral Pneumonia: 0.00%)\n",
      "**[Epoch 60]************************************************\n",
      "\tTraining results: Loss: 1.1208, Accuracy: 62.25% (COVID: 48.25%, Lung Opacity: 58.14%, Normal: 77.94%, Viral Pneumonia: 0.00%)\n",
      "\t Testing results: Loss: 1.1121, Accuracy: 62.30% (COVID: 51.99%, Lung Opacity: 55.89%, Normal: 77.62%, Viral Pneumonia: 0.00%)\n",
      "**[Epoch 80]************************************************\n",
      "\tTraining results: Loss: 1.1121, Accuracy: 63.06% (COVID: 50.92%, Lung Opacity: 57.32%, Normal: 79.17%, Viral Pneumonia: 0.00%)\n",
      "\t Testing results: Loss: 1.1130, Accuracy: 63.17% (COVID: 48.42%, Lung Opacity: 60.28%, Normal: 78.11%, Viral Pneumonia: 0.00%)\n",
      "**[Epoch 100]************************************************\n",
      "\tTraining results: Loss: 1.1059, Accuracy: 63.64% (COVID: 52.47%, Lung Opacity: 59.47%, Normal: 78.55%, Viral Pneumonia: 0.00%)\n",
      "\t Testing results: Loss: 1.1143, Accuracy: 62.82% (COVID: 51.72%, Lung Opacity: 62.27%, Normal: 75.02%, Viral Pneumonia: 0.00%)\n",
      "**[Epoch 120]************************************************\n",
      "\tTraining results: Loss: 1.1041, Accuracy: 63.77% (COVID: 52.34%, Lung Opacity: 58.49%, Normal: 79.44%, Viral Pneumonia: 0.00%)\n",
      "\t Testing results: Loss: 1.1068, Accuracy: 63.43% (COVID: 49.79%, Lung Opacity: 60.53%, Normal: 78.01%, Viral Pneumonia: 0.00%)\n",
      "**[Epoch 140]************************************************\n",
      "\tTraining results: Loss: 1.1032, Accuracy: 63.88% (COVID: 52.68%, Lung Opacity: 58.09%, Normal: 79.79%, Viral Pneumonia: 0.00%)\n",
      "\t Testing results: Loss: 1.1147, Accuracy: 63.36% (COVID: 51.44%, Lung Opacity: 58.71%, Normal: 78.35%, Viral Pneumonia: 0.00%)\n",
      "**[Epoch 160]************************************************\n",
      "\tTraining results: Loss: 1.1007, Accuracy: 64.20% (COVID: 53.20%, Lung Opacity: 59.57%, Normal: 79.39%, Viral Pneumonia: 0.00%)\n",
      "\t Testing results: Loss: 1.1058, Accuracy: 63.34% (COVID: 53.92%, Lung Opacity: 59.54%, Normal: 76.93%, Viral Pneumonia: 0.00%)\n",
      "**[Epoch 180]************************************************\n",
      "\tTraining results: Loss: 1.0993, Accuracy: 64.33% (COVID: 54.31%, Lung Opacity: 60.65%, Normal: 78.63%, Viral Pneumonia: 0.00%)\n",
      "\t Testing results: Loss: 1.1053, Accuracy: 63.45% (COVID: 47.87%, Lung Opacity: 61.77%, Normal: 78.01%, Viral Pneumonia: 0.00%)\n",
      "**[Epoch 200]************************************************\n",
      "\tTraining results: Loss: 1.0980, Accuracy: 64.46% (COVID: 54.38%, Lung Opacity: 59.65%, Normal: 79.47%, Viral Pneumonia: 0.00%)\n",
      "\t Testing results: Loss: 1.0960, Accuracy: 63.67% (COVID: 54.20%, Lung Opacity: 60.36%, Normal: 77.03%, Viral Pneumonia: 0.00%)\n",
      "**[Epoch 220]************************************************\n",
      "\tTraining results: Loss: 1.0964, Accuracy: 64.46% (COVID: 54.00%, Lung Opacity: 59.51%, Normal: 79.69%, Viral Pneumonia: 0.00%)\n",
      "\t Testing results: Loss: 1.0993, Accuracy: 63.62% (COVID: 53.23%, Lung Opacity: 59.45%, Normal: 77.82%, Viral Pneumonia: 0.00%)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-c1c7ae3c9221>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mwandb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#Train and evaluate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mtrainAndEvaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;31m#Finish WandB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-a0cc6082d071>\u001b[0m in \u001b[0;36mtrainAndEvaluate\u001b[1;34m(trainloader, testloader, model, criterion, optimizer)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;31m# Train the model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mtrainMetricsResults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainEpoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mprocessRunningMetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainMetricsResults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-028c459158ca>\u001b[0m in \u001b[0;36mtrainEpoch\u001b[1;34m(dataloader, model, criterion, optimizer)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;31m# Metrics for the training set.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0mupdateRunningMetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloaderLen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetricsResults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmetricsResults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-c3a084f1e13a>\u001b[0m in \u001b[0;36mupdateRunningMetrics\u001b[1;34m(outputs, groundtruth, loss, batchAmount, metricsResults)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmetricsResults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_loss\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatchAmount\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Accumulate the confusion matrix.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mconfusionMatrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetConfusionMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroundtruth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mmetricsResults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_confusionMatrix\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mconfusionMatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-9e07246e40ad>\u001b[0m in \u001b[0;36mgetConfusionMatrix\u001b[1;34m(outputs, groundtruth)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# Add 1 based on the prediction done for a specific label.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mconfusionMatrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroundtruth\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mconfusionMatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Create the model, optimizer and criterion\n",
    "trainloader, testloader = createDataLoaders()\n",
    "run_ID += 1\n",
    "model = MLP(config[inputSize], config[outputSize])\n",
    "optimizer = optim.Adam(model.parameters(),lr=config[learningRate])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#Init WandB\n",
    "name = \"MLP Steven \"+ str(run_ID)\n",
    "run = wandb.init(project='MLP', entity='tecai', config=config,\n",
    "                        name=name)\n",
    "wandb.watch(model)\n",
    "#Train and evaluate the model\n",
    "trainAndEvaluate(trainloader, testloader, model, criterion, optimizer)\n",
    "#Finish WandB\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}