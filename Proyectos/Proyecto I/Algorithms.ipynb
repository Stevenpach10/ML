{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python390jvsc74a57bd063fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d",
   "display_name": "Python 3.9.0 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Imports necesarios"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Models.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Hyperparameter tuning.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Metrics.\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_precision_recall_curve\n",
    "from sklearn.metrics import plot_roc_curve, roc_curve, roc_auc_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "source": [
    "### Miscellaneous functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a cut of the dataset, it assumes that there is going to be four files in the directory,\n",
    "#   called \"xTrain\", \"xTest\", \"yTrain\", \"yTest\".\n",
    "def readCut(dir):\n",
    "    xTrain = pd.read_csv(\"{}/xTrain.csv\".format(dir), sep=',', error_bad_lines=True, index_col=False, dtype='unicode')\n",
    "    yTrain = pd.read_csv(\"{}/yTrain.csv\".format(dir), sep=',', error_bad_lines=True, index_col=False, dtype='unicode')\n",
    "    xTest  = pd.read_csv( \"{}/xTest.csv\".format(dir), sep=',', error_bad_lines=True, index_col=False, dtype='unicode')\n",
    "    yTest  = pd.read_csv( \"{}/yTest.csv\".format(dir), sep=',', error_bad_lines=True, index_col=False, dtype='unicode')\n",
    "\n",
    "    return xTrain, yTrain, xTest, yTest\n",
    "\n",
    "# Entries names for the result dictionary.\n",
    "_accuracy   = \"Accuracy\"\n",
    "_precision  = \"Precision\"\n",
    "_recall     = \"Recall\"\n",
    "_f1         = \"F1\"\n",
    "_fpr        = \"FPR\"\n",
    "_tpr        = \"TPR\"\n",
    "_thresholds = \"Thresholds\"\n",
    "_auc        = \"AUC\"\n",
    "_roc        = \"ROC\"\n",
    "_model      = \"Model\"\n",
    "_min_iter   =   'min_cut_iter' \n",
    "_max_iter   =   'max_cut_iter' \n",
    "_model_min  =   'model_min'\n",
    "_model_max  =   'model_max'\n",
    "\n",
    "#Dictionary for visualization ROC\n",
    "_vis_minMetrics  =   'minMetrics' \n",
    "_vis_minModel    =   'minModel' \n",
    "_vis_maxMetrics  =   'maxMetrics'\n",
    "_vis_maxModel    =   'maxModel' \n",
    "_vis_pathMin     =   'pathMin'  \n",
    "_vis_pathMax     =   'pathMax'  \n",
    "\n",
    "# Metrics array.\n",
    "_metrics = [_accuracy, _precision, _recall, _f1]\n",
    "\n",
    "# Names of the feature enginereed datastes.\n",
    "_normalized   = \"Normalized\"\n",
    "_standardized = \"Standardized\"\n",
    "_betterFE     = \"BetterFE\"\n",
    "\n",
    "# Datasets array.\n",
    "#_datasets = [_normalized, _standardized, _betterFE]\n",
    "_datasets = [_normalized]\n",
    "\n",
    "# Entries names for the calculated results.\n",
    "_min  = \"Min\"\n",
    "_max  = \"Max\"\n",
    "_mean = \"Mean\"\n",
    "\n",
    "# Function to process the results of a model, to get a better print of them.\n",
    "def processAlgorithmResults(resultDictionary):\n",
    "\n",
    "    calculatedResultsDictionary = {}\n",
    "\n",
    "    # Process the results for every feature engineed dataset.\n",
    "    for dataset in _datasets:\n",
    "\n",
    "        # Set the current dataset entry.\n",
    "        calculatedResultsDictionary[dataset] = {}\n",
    "\n",
    "        # Process every metric.\n",
    "        for metric in _metrics:\n",
    "\n",
    "            # Store the metric values.\n",
    "            metricValues = []\n",
    "            classifiers = []\n",
    "            # Process the results for every cut.\n",
    "            for cut in resultDictionary[dataset]:\n",
    "\n",
    "                # Get the specific metric value for the current cut.\n",
    "                metricValues.append(cut[metric])\n",
    "                classifiers.append(cut[_model])\n",
    "\n",
    "            # Get the calculated results.\n",
    "            min_it = np.argmin(metricValues)\n",
    "            max_it = np.argmax(metricValues)\n",
    "            calculatedResultsDictionary[dataset][metric] = {\n",
    "                _min        : min(metricValues),\n",
    "                _max        : max(metricValues),\n",
    "                _mean       : sum(metricValues) / len(metricValues),\n",
    "                _min_iter   : int(min_it + 1),\n",
    "                _max_iter   : int(max_it + 1),\n",
    "                _model_min  : classifiers[min_it],\n",
    "                _model_max  : classifiers[max_it]\n",
    "            }\n",
    "\n",
    "    return calculatedResultsDictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotVis(diccionary):\n",
    "    xTrain, yTrain, xTest, yTest = readCut(pathMin)\n",
    "    for i in range (1,6):\n",
    "        if(len(diccionary[i]['minMetrics'])):\n",
    "            metrics = diccionary[i]['minMetrics']\n",
    "            model = diccionary[i]['minModel']\n",
    "            path = diccionary[i]['pathMin']\n",
    "\n",
    "            xTrain, yTrain, xTest, yTest = readCut(path)\n",
    "            print()\n",
    "            print(\"Roc plot for cut \" + str(i))\n",
    "            print(\"Model with min values for metrics:\")\n",
    "            print(metrics)\n",
    "            plot_roc_curve(model, xTest, yTest)\n",
    "            plt.show(block = False)\n",
    "\n",
    "        if(len(diccionary[i]['maxMetrics'])):\n",
    "            metrics = diccionary[i]['maxMetrics']\n",
    "            model = diccionary[i]['maxModel']\n",
    "            path = diccionary[i]['pathMax']\n",
    "\n",
    "            xTrain, yTrain, xTest, yTest = readCut(path)\n",
    "            print()\n",
    "            print(\"Roc plot for cut \" + str(i))\n",
    "            print(\"Model with max values for metrics:\")\n",
    "            print(metrics)\n",
    "            plot_roc_curve(model, xTest, yTest)\n",
    "            plt.show(block = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotROC(calculatedResults):\n",
    "        for dataset in _datasets:\n",
    "                print(\"Dataset : \" + dataset)\n",
    "                dicc_plot = {1   : {_vis_minMetrics : [],\n",
    "                                _vis_minModel : None,\n",
    "                                _vis_maxMetrics : [],\n",
    "                                _vis_maxModel : None,\n",
    "                                _vis_pathMin : '',\n",
    "                                _vis_pathMax : ''},\n",
    "                        2   : {_vis_minMetrics : [],\n",
    "                                _vis_minModel : None,\n",
    "                                _vis_maxMetrics : [],\n",
    "                                _vis_maxModel : None,\n",
    "                                _vis_pathMin : '',\n",
    "                                _vis_pathMax : ''},\n",
    "                        3   : {_vis_minMetrics : [],\n",
    "                                _vis_minModel : None,\n",
    "                                _vis_maxMetrics : [],\n",
    "                                _vis_maxModel : None,\n",
    "                                _vis_pathMin : '',\n",
    "                                _vis_pathMax : ''},\n",
    "                        4   : {_vis_minMetrics : [],\n",
    "                                _vis_minModel : None,\n",
    "                                _vis_maxMetrics : [],\n",
    "                                _vis_maxModel : None,\n",
    "                                _vis_pathMin : '',\n",
    "                                _vis_pathMax : ''},\n",
    "                        5   : {_vis_minMetrics : [],\n",
    "                                _vis_minModel : None,\n",
    "                                _vis_maxMetrics : [],\n",
    "                                _vis_maxModel : None,\n",
    "                                _vis_pathMin : '',\n",
    "                                _vis_pathMax : ''}\n",
    "                }\n",
    "                for metric in _metrics:\n",
    "                        cut_with_min_value  = calculatedResults[dataset][metric][_min_iter]\n",
    "                        model_min =calculatedResults[dataset][metric][_model_min]\n",
    "                        pathMin = os.path.join(dataset, \"cut\" + str(cut_with_min_value))\n",
    "\n",
    "                        dicc_plot[cut_with_min_value][_vis_minMetrics].append(metric)\n",
    "                        \n",
    "                        if(dicc_plot[cut_with_min_value][_vis_minModel] == None):\n",
    "                                dicc_plot[cut_with_min_value][_vis_minModel] = model_min\n",
    "                        dicc_plot[cut_with_min_value][_vis_pathMin] = pathMin\n",
    "\n",
    "\n",
    "                        cut_with_max_value  = calculatedResults[dataset][metric][_max_iter]\n",
    "                        model_max =calculatedResults[dataset][metric][_model_max]\n",
    "                        pathMax = os.path.join(dataset, \"cut\" + str(cut_with_max_value))\n",
    "\n",
    "                        dicc_plot[cut_with_max_value][_vis_maxMetrics].append(metric)\n",
    "                        \n",
    "                        if(dicc_plot[cut_with_max_value][_vis_maxModel] == None):\n",
    "                                dicc_plot[cut_with_max_value][_vis_maxModel] = model_max\n",
    "                        dicc_plot[cut_with_max_value][_vis_pathMax] = pathMax\n",
    "                plotVis(dicc_plot)\n"
   ]
  },
  {
   "source": [
    "# Definition of the models\n",
    "Each of the following functions is responsible for instantiating the model, defining the parameter grid to find the best parameters, and training the model with the best parameters to make predictions and calculate metrics."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doLogisticRegression(xTrain, yTrain, xTest, yTest):\n",
    "\n",
    "    # Model instantiation.\n",
    "    logisticRegression = LogisticRegression()\n",
    "\n",
    "    # Definition of the parameter grid.\n",
    "    param_grid = [{'penalty':['l2'],\n",
    "                'C' : np.logspace(-4, 4, 20),\n",
    "                'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "                'max_iter':[100, 500, 1000],\n",
    "                'fit_intercept' : [True, False]\n",
    "    }]\n",
    "    logreg_cv = GridSearchCV(logisticRegression, param_grid, n_jobs=-1)\n",
    "    \n",
    "    # Fit the model.\n",
    "    logreg_cv.fit(xTrain, yTrain.values.ravel())\n",
    "    #print(\"Tuned hyperparameters (best parameters): \", logreg_cv.best_params_)\n",
    "\n",
    "    # Predict using the model.\n",
    "    yPred = logreg_cv.predict(xTest)\n",
    "   \n",
    "\n",
    "    # Process the metrics.\n",
    "    dicResult = {\n",
    "       _accuracy   : accuracy_score(yTest.values.ravel(), yPred),\n",
    "       _precision  : precision_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _recall     : recall_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _f1         : f1_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _model      : logreg_cv\n",
    "    }\n",
    "\n",
    "    return dicResult\n"
   ]
  },
  {
   "source": [
    "### K-Nearest-Neighbor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doKNN(xTrain, yTrain, xTest, yTest):\n",
    "\n",
    "    # Model instantiation.\n",
    "    knn = KNeighborsClassifier()\n",
    "\n",
    "    # Definition of the parameter grid.\n",
    "    param_grid = [{'n_neighbors':[i for i in range(3,11)],\n",
    "                'weights' : ['uniform', 'distance'],\n",
    "                'algorithm' : ['ball_tree', 'kd_tree'],\n",
    "                'leaf_size' : [i for i in range(20,41)],\n",
    "                'metric' : ['euclidean', 'manhattan', 'chebyshev']\n",
    "    }]\n",
    "    knn_cv=GridSearchCV(knn, param_grid, n_jobs=-1)\n",
    "\n",
    "    # Fit the model.\n",
    "    knn_cv.fit(xTrain, yTrain.values.ravel())\n",
    "    #print(\"Tuned hyperparameters (best parameters): \", knn_cv.best_params_)\n",
    "\n",
    "    # Predict using the model.\n",
    "    yPred = knn_cv.predict(xTest)\n",
    "    _yPred = np.array([int(y) for y in yPred])\n",
    "\n",
    "    # Execute ROC.\n",
    "    fpr, tpr, thresh = roc_curve(yTest.values.ravel(), _yPred, pos_label='1')\n",
    "\n",
    "    # Process the metrics.\n",
    "    dicResult = {\n",
    "       _accuracy   : accuracy_score(yTest.values.ravel(), yPred),\n",
    "       _precision  : precision_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _recall     : recall_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _f1         : f1_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _fpr        : fpr,\n",
    "       _tpr        : tpr,\n",
    "       _thresholds : thresh,\n",
    "       _auc        : roc_auc_score(yTest.values.ravel(), yPred),\n",
    "       _model      : knn_cv\n",
    "    }\n",
    "\n",
    "    return dicResult"
   ]
  },
  {
   "source": [
    "### Decision Tree"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doDecisionTree(xTrain, yTrain, xTest, yTest):\n",
    "\n",
    "    # Model instantiation.\n",
    "    decisionTree = DecisionTreeClassifier()\n",
    "\n",
    "    # Definition of the parameter grid.\n",
    "    param_grid = [{'criterion':['gini', 'entropy'],\n",
    "                'max_depth': np.arange(3,15).tolist() + [None],\n",
    "                'splitter' : ['best', 'random'],\n",
    "                'max_features' : ['sqrt', 'log2', None]\n",
    "    }]\n",
    "    decisionTree_cv=GridSearchCV(decisionTree, param_grid, n_jobs=-1)\n",
    "\n",
    "    # Fit the model.\n",
    "    decisionTree_cv.fit(xTrain, yTrain.values.ravel())\n",
    "    #print(\"Tuned hyperparameters (best parameters): \", decisionTree_cv.best_params_)\n",
    "\n",
    "    # Predict using the model.\n",
    "    yPred = decisionTree_cv.predict(xTest)\n",
    "    _yPred = np.array([int(y) for y in yPred])\n",
    "\n",
    "    # Execute ROC.\n",
    "    fpr, tpr, thresh = roc_curve(yTest.values.ravel(), _yPred, pos_label='0')\n",
    "\n",
    "    # Process the metrics.\n",
    "    dicResult = {\n",
    "       _accuracy   : accuracy_score(yTest.values.ravel(), yPred),\n",
    "       _precision  : precision_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _recall     : recall_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _f1         : f1_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _fpr        : fpr,\n",
    "       _tpr        : tpr,\n",
    "       _thresholds : thresh,\n",
    "       _auc        : roc_auc_score(yTest.values.ravel(), yPred),\n",
    "       _model      : decisionTree_cv\n",
    "    }\n",
    "\n",
    "    return dicResult"
   ]
  },
  {
   "source": [
    "### Neural Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doNeuralNetwork(xTrain, yTrain, xTest, yTest):\n",
    "\n",
    "    # Model instantiation.\n",
    "    nnClassifier = MLPClassifier(max_iter=500)\n",
    "    \n",
    "    # Definition of the parameter grid.\n",
    "    # Use Saul's Heuristic for the amount of the hidden layers.\n",
    "    numberFeatures = xTrain.shape[1]\n",
    "    hiddenLayerAmount = np.arange(numberFeatures / 2, 2 * numberFeatures + 1)\n",
    "    param_grid =[{\n",
    "        'hidden_layer_sizes': [(10,30,10),(20,)],\n",
    "        'activation': ['tanh', 'relu','logistic'],\n",
    "        'solver': ['sgd', 'adam'],\n",
    "        'max_iter': [1000,1500],\n",
    "        'alpha': [0.0001, 0.05],\n",
    "    }]\n",
    "    nnClassifier_cv = GridSearchCV(nnClassifier, param_grid, n_jobs=-1)\n",
    "\n",
    "    # Fit the model.\n",
    "    nnClassifier_cv.fit(xTrain, yTrain.values.ravel())\n",
    "    #print(\"Tuned hyperparameters (best parameters): \", nnClassifier_cv.best_params_)\n",
    "\n",
    "    # Predict using the model.\n",
    "    yPred = nnClassifier_cv.predict(xTest)\n",
    "    _yPred = np.array([int(y) for y in yPred])\n",
    "\n",
    "    # Execute ROC.\n",
    "    fpr, tpr, thresh = roc_curve(yTest.values.ravel(), _yPred, pos_label='0')\n",
    "\n",
    "    # Process the metrics.\n",
    "    dicResult = {\n",
    "       _accuracy   : accuracy_score(yTest.values.ravel(), yPred),\n",
    "       _precision  : precision_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _recall     : recall_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _f1         : f1_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _auc        : roc_auc_score(yTest.values.ravel(), yPred),\n",
    "       _model      : nnClassifier_cv\n",
    "    }\n",
    "\n",
    "    return dicResult"
   ]
  },
  {
   "source": [
    "# Run algorithms\n",
    "We are going to run each algorithm for every cut for every feature engineered dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Logistic regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running logistic regression...\n",
      "Start for Normalized dataset\n",
      "\tDone cut: Normalized\\cut1\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-d78fe86988e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# Run logistic regression for the curren cut and get the metrics.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mpartialResult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxTest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myTest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;31m# Store the metrics for the current feature engineered dataset.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mlogisticRegressionResults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartialResult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-9487dfba4d43>\u001b[0m in \u001b[0;36mdoLogisticRegression\u001b[1;34m(xTrain, yTrain, xTest, yTest)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Fit the model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mlogreg_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myTrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;31m#print(\"Tuned hyperparameters (best parameters): \", logreg_cv.best_params_)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    839\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1286\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1288\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1054\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1055\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    931\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    433\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Running logistic regression...\")\n",
    "\n",
    "# Dictionary for the logistic regression results.\n",
    "logisticRegressionResults = {\n",
    "    _normalized : [],\n",
    "    _standardized : [],\n",
    "    _betterFE : []\n",
    "}\n",
    "\n",
    "# Run the algorithm for ...\n",
    "# For every feature engineered dataset.\n",
    "contar = 0\n",
    "for i in _datasets:\n",
    "    print(\"Start for\", i, \"dataset\")\n",
    "\n",
    "    # For every cut.\n",
    "    for j in range(1, 6):\n",
    "\n",
    "        # Build the dir.\n",
    "        path = os.path.join(i, \"cut\" + str(j))\n",
    "\n",
    "        # Get the cut from disk.\n",
    "        xTrain, yTrain, xTest, yTest = readCut(path)\n",
    "\n",
    "        # Run logistic regression for the curren cut and get the metrics.\n",
    "        partialResult = doLogisticRegression(xTrain, yTrain, xTest, yTest)\n",
    "        # Store the metrics for the current feature engineered dataset.\n",
    "        logisticRegressionResults[i].append(partialResult)\n",
    "\n",
    "        print(\"\\tDone cut:\", path)\n",
    "\n",
    "# Get the calculated mean, the min and the max for every metric.\n",
    "logisticRegressionCalculatedResults = processAlgorithmResults(logisticRegressionResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              Accuracy Precision    Recall        F1\nMin           0.703125  0.734463  0.695906  0.714715\nMax            0.80625  0.818713  0.818713  0.818713\nMean           0.74875  0.772022  0.752047   0.76162\nmin_cut_iter         1         2         1         1\nmax_cut_iter         4         4         4         4\n"
     ]
    }
   ],
   "source": [
    "# Print the results for every dataset.\n",
    "for dataset in _datasets:\n",
    "    # Create a panda frame to pretty print.\n",
    "    frame = pd.DataFrame(logisticRegressionCalculatedResults[dataset])\n",
    "    \n",
    "    #Delete models for pretty print\n",
    "    frame.drop(_model_min, inplace=True)\n",
    "    frame.drop(_model_max, inplace=True)\n",
    "    # Print the frame.\n",
    "    print(frame)"
   ]
  },
  {
   "source": [
    "Plot ROC - Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotROC(logisticRegressionCalculatedResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}