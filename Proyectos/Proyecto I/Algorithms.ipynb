{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "45beaf1bbaf1f4539db6cc65708e9bc09be995c5454858dbd2322c7e6d854a88"
    }
   }
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Imports necesarios"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Models.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Hyperparameter tuning.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Metrics.\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "source": [
    "### Miscellaneous functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a cut of the dataset, it assumes that there is going to be four files in the directory,\n",
    "#   called \"xTrain\", \"xTest\", \"yTrain\", \"yTest\".\n",
    "def readCut(dir):\n",
    "    xTrain = pd.read_csv(\"{}/xTrain.csv\".format(dir), sep=',', error_bad_lines=True, index_col=False, dtype='unicode')\n",
    "    yTrain = pd.read_csv(\"{}/yTrain.csv\".format(dir), sep=',', error_bad_lines=True, index_col=False, dtype='unicode')\n",
    "    xTest  = pd.read_csv( \"{}/xTest.csv\".format(dir), sep=',', error_bad_lines=True, index_col=False, dtype='unicode')\n",
    "    yTest  = pd.read_csv( \"{}/yTest.csv\".format(dir), sep=',', error_bad_lines=True, index_col=False, dtype='unicode')\n",
    "\n",
    "    return xTrain, yTrain, xTest, yTest\n",
    "\n",
    "# Entries names for the result dictionary.\n",
    "_accuracy   = \"Accuracy\"\n",
    "_precision  = \"Precision\"\n",
    "_recall     = \"Recall\"\n",
    "_f1         = \"F1\"\n",
    "_fpr        = \"FPR\"\n",
    "_tpr        = \"TPR\"\n",
    "_thresholds = \"Thresholds\"\n",
    "_auc        = \"AUC\"\n",
    "_model      = \"Model\"\n",
    "\n",
    "# Metrics array.\n",
    "_metrics = [_accuracy, _precision, _recall, _f1]\n",
    "\n",
    "# Names of the feature enginereed datastes.\n",
    "_normalized   = \"Normalized\"\n",
    "_standardized = \"Standardized\"\n",
    "_betterFE     = \"BetterFE\"\n",
    "\n",
    "# Datasets array.\n",
    "_datasets = [_normalized, _standardized, _betterFE]\n",
    "\n",
    "# Entries names for the calculated results.\n",
    "_min  = \"Min\"\n",
    "_max  = \"Max\"\n",
    "_mean = \"Mean\"\n",
    "\n",
    "# Function to process the results of a model, to get a better print of them.\n",
    "def processAlgorithmResults(resultDictionary):\n",
    "\n",
    "    calculatedResultsDictionary = {}\n",
    "\n",
    "    # Process the results for every feature engineed dataset.\n",
    "    for dataset in _datasets:\n",
    "\n",
    "        # Set the current dataset entry.\n",
    "        calculatedResultsDictionary[dataset] = {}\n",
    "\n",
    "        # Process every metric.\n",
    "        for metric in _metrics:\n",
    "\n",
    "            # Store the metric values.\n",
    "            metricValues = []\n",
    "\n",
    "            # Process the results for every cut.\n",
    "            for cut in resultDictionary[dataset]:\n",
    "\n",
    "                # Get the specific metric value for the current cut.\n",
    "                metricValues.append(cut[metric])\n",
    "\n",
    "            # Get the calculated results.\n",
    "            calculatedResultsDictionary[dataset][metric] = {\n",
    "                _min : min(metricValues),\n",
    "                _max : max(metricValues),\n",
    "                _mean : sum(metricValues) / len(metricValues)\n",
    "            }\n",
    "\n",
    "    return calculatedResultsDictionary"
   ]
  },
  {
   "source": [
    "# Definition of the models\n",
    "Each of the following functions is responsible for instantiating the model, defining the parameter grid to find the best parameters, and training the model with the best parameters to make predictions and calculate metrics."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doLogisticRegression(xTrain, yTrain, xTest, yTest):\n",
    "\n",
    "    # Model instantiation.\n",
    "    logisticRegression = LogisticRegression()\n",
    "\n",
    "    # Definition of the parameter grid.\n",
    "    param_grid = [{'penalty':['l2'],\n",
    "                'C' : np.logspace(-4, 4, 20),\n",
    "                'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "                'max_iter':[100, 500, 1000],\n",
    "                'fit_intercept' : [True, False]\n",
    "    }]\n",
    "    logreg_cv = GridSearchCV(logisticRegression, param_grid, n_jobs=-1)\n",
    "    \n",
    "    # Fit the model.\n",
    "    logreg_cv.fit(xTrain, yTrain.values.ravel())\n",
    "    #print(\"Tuned hyperparameters (best parameters): \", logreg_cv.best_params_)\n",
    "\n",
    "    # Predict using the model.\n",
    "    yPred = logreg_cv.predict(xTest)\n",
    "    _yPred = np.array([int(y) for y in yPred])\n",
    "\n",
    "    # Execute ROC.\n",
    "    fpr, tpr, thresh = roc_curve(yTest.values.ravel(), yPred, pos_label='0')\n",
    "    print(yTest, yPred)\n",
    "\n",
    "    # Process the metrics.\n",
    "    dicResult = {\n",
    "       _accuracy   : accuracy_score(yTest.values.ravel(), yPred),\n",
    "       _precision  : precision_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _recall     : recall_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _f1         : f1_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _fpr        : fpr,\n",
    "       _tpr        : tpr,\n",
    "       _thresholds : thresh,\n",
    "       _auc        : roc_auc_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _model      : logreg_cv\n",
    "    }\n",
    "\n",
    "    return dicResult\n"
   ]
  },
  {
   "source": [
    "### K-Nearest-Neighbor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doKNN(xTrain, yTrain, xTest, yTest):\n",
    "\n",
    "    # Model instantiation.\n",
    "    knn = KNeighborsClassifier()\n",
    "\n",
    "    # Definition of the parameter grid.\n",
    "    param_grid = [{'n_neighbors':[i for i in range(3,11)],\n",
    "                'weights' : ['uniform', 'distance'],\n",
    "                'algorithm' : ['ball_tree', 'kd_tree'],\n",
    "                'leaf_size' : [i for i in range(20,41)],\n",
    "                'metric' : ['euclidean', 'manhattan', 'chebyshev']\n",
    "    }]\n",
    "    knn_cv=GridSearchCV(knn, param_grid, n_jobs=-1)\n",
    "\n",
    "    # Fit the model.\n",
    "    knn_cv.fit(xTrain, yTrain.values.ravel())\n",
    "    #print(\"Tuned hyperparameters (best parameters): \", knn_cv.best_params_)\n",
    "\n",
    "    # Predict using the model.\n",
    "    yPred = knn_cv.predict(xTest)\n",
    "\n",
    "    # Execute ROC.\n",
    "    fpr, tpr, thresh = roc_curve(yTest.values.ravel(), yPred)\n",
    "\n",
    "    # Process the metrics.\n",
    "    dicResult = {\n",
    "       _accuracy   : accuracy_score(yTest.values.ravel(), yPred),\n",
    "       _precision  : precision_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _recall     : recall_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _f1         : f1_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _fpr        : fpr,\n",
    "       _tpr        : tpr,\n",
    "       _thresholds : thresh,\n",
    "       _auc        : roc_auc_score(yTest.values.ravel(), yPred),\n",
    "       _model      : knn_cv\n",
    "    }\n",
    "\n",
    "    return dicResult"
   ]
  },
  {
   "source": [
    "### Decision Tree"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doDecisionTree(xTrain, yTrain, xTest, yTest):\n",
    "\n",
    "    # Model instantiation.\n",
    "    decisionTree = DecisionTreeClassifier()\n",
    "\n",
    "    # Definition of the parameter grid.\n",
    "    param_grid = [{'criterion':['gini', 'entropy'],\n",
    "                'max_depth': np.arange(3,15).tolist() + [None],\n",
    "                'splitter' : ['best', 'random'],\n",
    "                'max_features' : ['sqrt', 'log2', None]\n",
    "    }]\n",
    "    decisionTree_cv=GridSearchCV(decisionTree, param_grid, n_jobs=-1)\n",
    "\n",
    "    # Fit the model.\n",
    "    decisionTree_cv.fit(xTrain, yTrain.values.ravel())\n",
    "    #print(\"Tuned hyperparameters (best parameters): \", decisionTree_cv.best_params_)\n",
    "\n",
    "    # Predict using the model.\n",
    "    yPred = decisionTree_cv.predict(xTest)\n",
    "\n",
    "    # Execute ROC.\n",
    "    fpr, tpr, thresh = roc_curve(yTest.values.ravel(), yPred)\n",
    "\n",
    "    # Process the metrics.\n",
    "    dicResult = {\n",
    "       _accuracy   : accuracy_score(yTest.values.ravel(), yPred),\n",
    "       _precision  : precision_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _recall     : recall_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _f1         : f1_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _fpr        : fpr,\n",
    "       _tpr        : tpr,\n",
    "       _thresholds : thresh,\n",
    "       _auc        : roc_auc_score(yTest.values.ravel(), yPred),\n",
    "       _model      : decisionTree_cv\n",
    "    }\n",
    "\n",
    "    return dicResult"
   ]
  },
  {
   "source": [
    "### Neural Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doNeuralNetwork(xTrain, yTrain, xTest, yTest):\n",
    "\n",
    "    # Model instantiation.\n",
    "    nnClassifier = MLPClassifier(max_iter=500)\n",
    "    \n",
    "    # Definition of the parameter grid.\n",
    "    # Use Saul's Heuristic for the amount of the hidden layers.\n",
    "    numberFeatures = xTrain.shape[1]\n",
    "    hiddenLayerAmount = np.arange(numberFeatures / 2, 2 * numberFeatures + 1)\n",
    "    param_grid =[{\n",
    "        'hidden_layer_sizes': [(10,30,10),(20,)],\n",
    "        'activation': ['tanh', 'relu','logistic'],\n",
    "        'solver': ['sgd', 'adam'],\n",
    "        'max_iter': [1000,1500],\n",
    "        'alpha': [0.0001, 0.05],\n",
    "    }]\n",
    "    nnClassifier_cv = GridSearchCV(nnClassifier, param_grid, n_jobs=-1)\n",
    "\n",
    "    # Fit the model.\n",
    "    nnClassifier_cv.fit(xTrain, yTrain.values.ravel())\n",
    "    #print(\"Tuned hyperparameters (best parameters): \", nnClassifier_cv.best_params_)\n",
    "\n",
    "    # Predict using the model.\n",
    "    yPred = nnClassifier_cv.predict(xTest)\n",
    "\n",
    "    # Execute ROC.\n",
    "    fpr, tpr, thresh = roc_curve(yTest.values.ravel(), yPred)\n",
    "\n",
    "    # Process the metrics.\n",
    "    dicResult = {\n",
    "       _accuracy   : accuracy_score(yTest.values.ravel(), yPred),\n",
    "       _precision  : precision_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _recall     : recall_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _f1         : f1_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _fpr        : fpr,\n",
    "       _tpr        : tpr,\n",
    "       _thresholds : thresh,\n",
    "       _auc        : roc_auc_score(yTest.values.ravel(), yPred),\n",
    "       _model      : nnClassifier_cv\n",
    "    }\n",
    "\n",
    "    return dicResult"
   ]
  },
  {
   "source": [
    "# Run algorithms\n",
    "We are going to run each algorithm for every cut for every feature engineered dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Logistic regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running logistic regression...\nStart for Normalized dataset\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'str'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-118-952646728536>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# Run logistic regression for the curren cut and get the metrics.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mpartialResult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxTest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myTest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# Store the metrics for the current feature engineered dataset.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-114-825f1ceda4fc>\u001b[0m in \u001b[0;36mdoLogisticRegression\u001b[1;34m(xTrain, yTrain, xTest, yTest)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# Execute ROC.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mfpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthresh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myTest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myPred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myTest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myPred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m     \"\"\"\n\u001b[1;32m--> 775\u001b[1;33m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0m\u001b[0;32m    776\u001b[0m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0;32m    777\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[1;31m# the indices associated with the distinct values. We also\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m     \u001b[1;31m# concatenate a value for the end of the curve.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[0mdistinct_value_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m     \u001b[0mthreshold_idxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mr_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdistinct_value_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdiff\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mdiff\u001b[1;34m(a, n, axis, prepend, append)\u001b[0m\n\u001b[0;32m   1278\u001b[0m     \u001b[0mop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnot_equal\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool_\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msubtract\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1280\u001b[1;33m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mslice1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mslice2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1282\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "print(\"Running logistic regression...\")\n",
    "\n",
    "# Dictionary for the logistic regression results.\n",
    "logisticRegressionResults = {\n",
    "    _normalized : [],\n",
    "    _standardized : [],\n",
    "    _betterFE : []\n",
    "}\n",
    "\n",
    "# Run the algorithm for ...\n",
    "# For every feature engineered dataset.\n",
    "for i in _datasets:\n",
    "\n",
    "    print(\"Start for\", i, \"dataset\")\n",
    "\n",
    "    # For every cut.\n",
    "    for j in range(1, 6):\n",
    "\n",
    "        # Build the dir.\n",
    "        path = os.path.join(i, \"cut\" + str(j))\n",
    "\n",
    "        # Get the cut from disk.\n",
    "        xTrain, yTrain, xTest, yTest = readCut(path)\n",
    "\n",
    "        # Run logistic regression for the curren cut and get the metrics.\n",
    "        partialResult = doLogisticRegression(xTrain, yTrain, xTest, yTest)\n",
    "\n",
    "        # Store the metrics for the current feature engineered dataset.\n",
    "        logisticRegressionResults[i].append(partialResult)\n",
    "\n",
    "        print(\"\\tDone cut:\", path)\n",
    "\n",
    "# Get the calculated mean, the min and the max for every metric.\n",
    "logisticRegressionCalculatedResults = processAlgorithmResults(logisticRegressionResults)\n",
    "\n",
    "# Print the results for every dataset.\n",
    "for dataset in _datasets:\n",
    "\n",
    "    print()\n",
    "    print(dataset, \"dataset metrics result.\")\n",
    "\n",
    "    # Create a panda frame to pretty print.\n",
    "    frame = pd.DataFrame(logisticRegressionCalculatedResults[dataset])\n",
    "    # Print the frame.\n",
    "    print(frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Done!\")"
   ]
  }
 ]
}