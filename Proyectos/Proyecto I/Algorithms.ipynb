{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd045beaf1bbaf1f4539db6cc65708e9bc09be995c5454858dbd2322c7e6d854a88",
   "display_name": "Python 3.8.5 64-bit (conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Imports necesarios"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Models.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Hyperparameter tuning.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Metrics.\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_precision_recall_curve\n",
    "from sklearn.metrics import plot_roc_curve, roc_curve, roc_auc_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "source": [
    "### Miscellaneous functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a cut of the dataset, it assumes that there is going to be four files in the directory,\n",
    "#   called \"xTrain\", \"xTest\", \"yTrain\", \"yTest\".\n",
    "def readCut(dir):\n",
    "    xTrain = pd.read_csv(\"{}/xTrain.csv\".format(dir), sep=',', error_bad_lines=True, index_col=False, dtype='unicode')\n",
    "    yTrain = pd.read_csv(\"{}/yTrain.csv\".format(dir), sep=',', error_bad_lines=True, index_col=False, dtype='unicode')\n",
    "    xTest  = pd.read_csv( \"{}/xTest.csv\".format(dir), sep=',', error_bad_lines=True, index_col=False, dtype='unicode')\n",
    "    yTest  = pd.read_csv( \"{}/yTest.csv\".format(dir), sep=',', error_bad_lines=True, index_col=False, dtype='unicode')\n",
    "\n",
    "    return xTrain, yTrain, xTest, yTest\n",
    "\n",
    "# Entries names for the result dictionary.\n",
    "_accuracy   = \"Accuracy\"\n",
    "_precision  = \"Precision\"\n",
    "_recall     = \"Recall\"\n",
    "_f1         = \"F1\"\n",
    "_fpr        = \"FPR\"\n",
    "_tpr        = \"TPR\"\n",
    "_thresholds = \"Thresholds\"\n",
    "_auc        = \"AUC\"\n",
    "_roc        = \"ROC\"\n",
    "_model      = \"Model\"\n",
    "\n",
    "# Metrics array.\n",
    "_metrics = [_accuracy, _precision, _recall, _f1]\n",
    "\n",
    "# Names of the feature enginereed datastes.\n",
    "_normalized   = \"Normalized\"\n",
    "_standardized = \"Standardized\"\n",
    "_betterFE     = \"BetterFE\"\n",
    "\n",
    "# Datasets array.\n",
    "_datasets = [_normalized, _standardized, _betterFE]\n",
    "\n",
    "# Entries names for the calculated results.\n",
    "_min  = \"Min\"\n",
    "_max  = \"Max\"\n",
    "_mean = \"Mean\"\n",
    "\n",
    "# Function to process the results of a model, to get a better print of them.\n",
    "def processAlgorithmResults(resultDictionary):\n",
    "\n",
    "    calculatedResultsDictionary = {}\n",
    "\n",
    "    # Process the results for every feature engineed dataset.\n",
    "    for dataset in _datasets:\n",
    "\n",
    "        # Set the current dataset entry.\n",
    "        calculatedResultsDictionary[dataset] = {}\n",
    "\n",
    "        # Process every metric.\n",
    "        for metric in _metrics:\n",
    "\n",
    "            # Store the metric values.\n",
    "            metricValues = []\n",
    "\n",
    "            # Process the results for every cut.\n",
    "            for cut in resultDictionary[dataset]:\n",
    "\n",
    "                # Get the specific metric value for the current cut.\n",
    "                metricValues.append(cut[metric])\n",
    "\n",
    "            # Get the calculated results.\n",
    "            calculatedResultsDictionary[dataset][metric] = {\n",
    "                _min : min(metricValues),\n",
    "                _max : max(metricValues),\n",
    "                _mean : sum(metricValues) / len(metricValues)\n",
    "            }\n",
    "\n",
    "    return calculatedResultsDictionary"
   ]
  },
  {
   "source": [
    "# Definition of the models\n",
    "Each of the following functions is responsible for instantiating the model, defining the parameter grid to find the best parameters, and training the model with the best parameters to make predictions and calculate metrics."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doLogisticRegression(xTrain, yTrain, xTest, yTest):\n",
    "\n",
    "    # Model instantiation.\n",
    "    logisticRegression = LogisticRegression()\n",
    "\n",
    "    # Definition of the parameter grid.\n",
    "    param_grid = [{'penalty':['l2'],\n",
    "                'C' : np.logspace(-4, 4, 20),\n",
    "                'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "                'max_iter':[100, 500, 1000],\n",
    "                'fit_intercept' : [True, False]\n",
    "    }]\n",
    "    logreg_cv = GridSearchCV(logisticRegression, param_grid, n_jobs=-1)\n",
    "    \n",
    "    # Fit the model.\n",
    "    logreg_cv.fit(xTrain, yTrain.values.ravel())\n",
    "    #print(\"Tuned hyperparameters (best parameters): \", logreg_cv.best_params_)\n",
    "\n",
    "    # Predict using the model.\n",
    "    yPred = logreg_cv.predict(xTest)\n",
    "    _yPred = np.array([int(y) for y in yPred])\n",
    "\n",
    "    # Execute ROC.\n",
    "    fpr, tpr, thresh = roc_curve(yTest.values.ravel(), _yPred, pos_label='1')\n",
    "    roc = plot_roc_curve(logreg_cv, xTest, yTest)\n",
    "\n",
    "    # Process the metrics.\n",
    "    dicResult = {\n",
    "       _accuracy   : accuracy_score(yTest.values.ravel(), yPred),\n",
    "       _precision  : precision_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _recall     : recall_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _f1         : f1_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _fpr        : fpr,\n",
    "       _tpr        : tpr,\n",
    "       _thresholds : thresh,\n",
    "       _auc        : roc_auc_score(yTest.values.ravel(), yPred),\n",
    "       _roc        : roc,\n",
    "       _model      : logreg_cv\n",
    "    }\n",
    "\n",
    "    return dicResult\n"
   ]
  },
  {
   "source": [
    "### K-Nearest-Neighbor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doKNN(xTrain, yTrain, xTest, yTest):\n",
    "\n",
    "    # Model instantiation.\n",
    "    knn = KNeighborsClassifier()\n",
    "\n",
    "    # Definition of the parameter grid.\n",
    "    param_grid = [{'n_neighbors':[i for i in range(3,11)],\n",
    "                'weights' : ['uniform', 'distance'],\n",
    "                'algorithm' : ['ball_tree', 'kd_tree'],\n",
    "                'leaf_size' : [i for i in range(20,41)],\n",
    "                'metric' : ['euclidean', 'manhattan', 'chebyshev']\n",
    "    }]\n",
    "    knn_cv=GridSearchCV(knn, param_grid, n_jobs=-1)\n",
    "\n",
    "    # Fit the model.\n",
    "    knn_cv.fit(xTrain, yTrain.values.ravel())\n",
    "    #print(\"Tuned hyperparameters (best parameters): \", knn_cv.best_params_)\n",
    "\n",
    "    # Predict using the model.\n",
    "    yPred = knn_cv.predict(xTest)\n",
    "    _yPred = np.array([int(y) for y in yPred])\n",
    "\n",
    "    # Execute ROC.\n",
    "    fpr, tpr, thresh = roc_curve(yTest.values.ravel(), _yPred, pos_label='1')\n",
    "\n",
    "    # Process the metrics.\n",
    "    dicResult = {\n",
    "       _accuracy   : accuracy_score(yTest.values.ravel(), yPred),\n",
    "       _precision  : precision_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _recall     : recall_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _f1         : f1_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _fpr        : fpr,\n",
    "       _tpr        : tpr,\n",
    "       _thresholds : thresh,\n",
    "       _auc        : roc_auc_score(yTest.values.ravel(), yPred),\n",
    "       _model      : knn_cv\n",
    "    }\n",
    "\n",
    "    return dicResult"
   ]
  },
  {
   "source": [
    "### Decision Tree"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doDecisionTree(xTrain, yTrain, xTest, yTest):\n",
    "\n",
    "    # Model instantiation.\n",
    "    decisionTree = DecisionTreeClassifier()\n",
    "\n",
    "    # Definition of the parameter grid.\n",
    "    param_grid = [{'criterion':['gini', 'entropy'],\n",
    "                'max_depth': np.arange(3,15).tolist() + [None],\n",
    "                'splitter' : ['best', 'random'],\n",
    "                'max_features' : ['sqrt', 'log2', None]\n",
    "    }]\n",
    "    decisionTree_cv=GridSearchCV(decisionTree, param_grid, n_jobs=-1)\n",
    "\n",
    "    # Fit the model.\n",
    "    decisionTree_cv.fit(xTrain, yTrain.values.ravel())\n",
    "    #print(\"Tuned hyperparameters (best parameters): \", decisionTree_cv.best_params_)\n",
    "\n",
    "    # Predict using the model.\n",
    "    yPred = decisionTree_cv.predict(xTest)\n",
    "    _yPred = np.array([int(y) for y in yPred])\n",
    "\n",
    "    # Execute ROC.\n",
    "    fpr, tpr, thresh = roc_curve(yTest.values.ravel(), _yPred, pos_label='0')\n",
    "\n",
    "    # Process the metrics.\n",
    "    dicResult = {\n",
    "       _accuracy   : accuracy_score(yTest.values.ravel(), yPred),\n",
    "       _precision  : precision_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _recall     : recall_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _f1         : f1_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _fpr        : fpr,\n",
    "       _tpr        : tpr,\n",
    "       _thresholds : thresh,\n",
    "       _auc        : roc_auc_score(yTest.values.ravel(), yPred),\n",
    "       _model      : decisionTree_cv\n",
    "    }\n",
    "\n",
    "    return dicResult"
   ]
  },
  {
   "source": [
    "### Neural Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doNeuralNetwork(xTrain, yTrain, xTest, yTest):\n",
    "\n",
    "    # Model instantiation.\n",
    "    nnClassifier = MLPClassifier(max_iter=500)\n",
    "    \n",
    "    # Definition of the parameter grid.\n",
    "    # Use Saul's Heuristic for the amount of the hidden layers.\n",
    "    numberFeatures = xTrain.shape[1]\n",
    "    hiddenLayerAmount = np.arange(numberFeatures / 2, 2 * numberFeatures + 1)\n",
    "    param_grid =[{\n",
    "        'hidden_layer_sizes': [(10,30,10),(20,)],\n",
    "        'activation': ['tanh', 'relu','logistic'],\n",
    "        'solver': ['sgd', 'adam'],\n",
    "        'max_iter': [1000,1500],\n",
    "        'alpha': [0.0001, 0.05],\n",
    "    }]\n",
    "    nnClassifier_cv = GridSearchCV(nnClassifier, param_grid, n_jobs=-1)\n",
    "\n",
    "    # Fit the model.\n",
    "    nnClassifier_cv.fit(xTrain, yTrain.values.ravel())\n",
    "    #print(\"Tuned hyperparameters (best parameters): \", nnClassifier_cv.best_params_)\n",
    "\n",
    "    # Predict using the model.\n",
    "    yPred = nnClassifier_cv.predict(xTest)\n",
    "    _yPred = np.array([int(y) for y in yPred])\n",
    "\n",
    "    # Execute ROC.\n",
    "    fpr, tpr, thresh = roc_curve(yTest.values.ravel(), _yPred, pos_label='0')\n",
    "\n",
    "    # Process the metrics.\n",
    "    dicResult = {\n",
    "       _accuracy   : accuracy_score(yTest.values.ravel(), yPred),\n",
    "       _precision  : precision_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _recall     : recall_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _f1         : f1_score(yTest.values.ravel(), yPred, pos_label='1'),\n",
    "       _fpr        : fpr,\n",
    "       _tpr        : tpr,\n",
    "       _thresholds : thresh,\n",
    "       _auc        : roc_auc_score(yTest.values.ravel(), yPred),\n",
    "       _model      : nnClassifier_cv\n",
    "    }\n",
    "\n",
    "    return dicResult"
   ]
  },
  {
   "source": [
    "# Run algorithms\n",
    "We are going to run each algorithm for every cut for every feature engineered dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Logistic regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Running logistic regression...\")\n",
    "\n",
    "# Dictionary for the logistic regression results.\n",
    "logisticRegressionResults = {\n",
    "    _normalized : [],\n",
    "    _standardized : [],\n",
    "    _betterFE : []\n",
    "}\n",
    "\n",
    "# Run the algorithm for ...\n",
    "# For every feature engineered dataset.\n",
    "for i in _datasets:\n",
    "\n",
    "    print(\"Start for\", i, \"dataset\")\n",
    "\n",
    "    # For every cut.\n",
    "    for j in range(1, 6):\n",
    "\n",
    "        # Build the dir.\n",
    "        path = os.path.join(i, \"cut\" + str(j))\n",
    "\n",
    "        # Get the cut from disk.\n",
    "        xTrain, yTrain, xTest, yTest = readCut(path)\n",
    "\n",
    "        # Run logistic regression for the curren cut and get the metrics.\n",
    "        partialResult = doLogisticRegression(xTrain, yTrain, xTest, yTest)\n",
    "\n",
    "        # Store the metrics for the current feature engineered dataset.\n",
    "        logisticRegressionResults[i].append(partialResult)\n",
    "\n",
    "        print(\"\\tDone cut:\", path)\n",
    "\n",
    "# Get the calculated mean, the min and the max for every metric.\n",
    "logisticRegressionCalculatedResults = processAlgorithmResults(logisticRegressionResults)\n",
    "\n",
    "# Print the results for every dataset.\n",
    "for dataset in _datasets:\n",
    "\n",
    "    print()\n",
    "    print(dataset, \"dataset metrics result.\")\n",
    "\n",
    "    # Create a panda frame to pretty print.\n",
    "    frame = pd.DataFrame(logisticRegressionCalculatedResults[dataset])\n",
    "    # Print the frame.\n",
    "    print(frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__FPR = logisticRegressionResults['Normalized'][0]['FPR']\n",
    "__TPR = logisticRegressionResults['Normalized'][0]['TPR']\n",
    "__AUC = logisticRegressionResults['Normalized'][0]['AUC']\n",
    "\n",
    "display = metrics.RocCurveDisplay(fpr=__FPR, tpr=__TPR, roc_auc=__AUC, estimator_name='example estimator')\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done!\")"
   ]
  }
 ]
}