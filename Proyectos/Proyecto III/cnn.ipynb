{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd009cf3d5909f3828bf53c46564821f879a9c8405e40e27fb09b54d745264e4a6d",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "09cf3d5909f3828bf53c46564821f879a9c8405e40e27fb09b54d745264e4a6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General import of torch.\n",
    "import torch\n",
    "# Import for graph blocks of torch.\n",
    "import torch.nn as nn\n",
    "# Import the models library, to get the model to be used.\n",
    "import torchvision.models as models\n",
    "# Import optim library, to get the optimizer to be used.\n",
    "import torch.optim as optim\n",
    "# Import torchvision, to manage the input data.\n",
    "import torchvision\n",
    "# To apply transformations to the data (when loaded).\n",
    "import torchvision.transforms as transforms\n",
    "# To calculate softmax.\n",
    "from torch.nn import Softmax\n",
    "softmax = Softmax(dim=1)\n",
    "\n",
    "# Metrics zero_division\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# General imports.\n",
    "import os\n",
    "import time\n",
    "import wandb\n",
    "import random\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path"
   ]
  },
  {
   "source": [
    "### Enviroment configuration."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input size of the model.\n",
    "inputSize = 'inputSize'\n",
    "# Output size of the model.\n",
    "outputSize = 'outputSize'\n",
    "# Batch size.\n",
    "batchSize = 'batchSize'\n",
    "# Epochs amount.\n",
    "epochs = 'epochs'\n",
    "# Learning rate.\n",
    "learningRate = 'learningRate'\n",
    "# Class names.\n",
    "classes = 'classes'\n",
    "# Class IDs.\n",
    "classesIDs = 'classesIDs'\n",
    "# Number of classes to classify.\n",
    "classesLen = 'classesLen'\n",
    "\n",
    "config = {\n",
    "    inputSize    : 224,\n",
    "    outputSize   : 39,\n",
    "    batchSize    : 128,\n",
    "    epochs       : 200,\n",
    "    learningRate : 0.001,\n",
    "    classes : [\n",
    "        'Apple - Apple scab',\n",
    "        'Apple - Black rot',\n",
    "        'Apple - Cedar apple rust',\n",
    "        'Apple - Healthy',\n",
    "        'Background without leaves',\n",
    "        'Blueberry - Healthy',\n",
    "        'Cherry - Healthy',\n",
    "        'Cherry - Powdery mildew',\n",
    "        'Corn - Cercospora',\n",
    "        'Corn - Common rust',\n",
    "        'Corn - Healthy',\n",
    "        'Corn - Northern Leaf Blight',\n",
    "        'Grape - Black rot',\n",
    "        'Grape - Esca',\n",
    "        'Grape - Healthy',\n",
    "        'Grape - Leaf blight',\n",
    "        'Orange - Haunglongbing',\n",
    "        'Peach - Bacterial spot',\n",
    "        'Peach - Healthy',\n",
    "        'Pepper bell - Bacterial spot',\n",
    "        'Pepper bell - healthy',\n",
    "        'Potato - Early blight',\n",
    "        'Potato - Healthy',\n",
    "        'Potato - Late blight',\n",
    "        'Raspberry - healthy',\n",
    "        'Soybean - Healthy',\n",
    "        'Squash - Powdery mildew',\n",
    "        'Strawberry - Healthy',\n",
    "        'Strawberry - Leaf scorch',\n",
    "        'Tomato - Bacterial spot',\n",
    "        'Tomato - Early blight',\n",
    "        'Tomato - Healthy',\n",
    "        'Tomato - Late blight',\n",
    "        'Tomato - Leaf Mold',\n",
    "        'Tomato - Septoria leaf spot',\n",
    "        'Tomato - Spider mites',\n",
    "        'Tomato - Target Spot',\n",
    "        'Tomato - Mosaic virus',\n",
    "        'Tomato - Yellow Leaf Curl Virus'\n",
    "    ],\n",
    "    classesIDs : [i for i in range(39)],\n",
    "    classesLen : 39\n",
    "}\n",
    "\n",
    "# Device to be used, prefer cuda, if available.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "source": [
    "### Miscellaneous functions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# About metrics.\n",
    "# Metric dictionary keys.\n",
    "# For preprocessing.\n",
    "_loss        = 'Loss'\n",
    "_groundtruth = 'Groundtruth'\n",
    "_logits      = 'Logits'\n",
    "# For postprocessing.\n",
    "_probabilities   = 'Probabilities'\n",
    "_predictions     = 'Predictions'\n",
    "_accuracyClass   = 'Accuracy class'\n",
    "_accuracy        = 'Accuracy'\n",
    "_recall          = 'Recall'\n",
    "_precision       = 'Precision'\n",
    "_f1              = 'F1'\n",
    "_auc             = 'AUC'\n",
    "# For torch save\n",
    "_model           = 'Model State Dic'\n",
    "_optimizer       = 'Optimizer State Dic'\n",
    "_epoch           = 'Epoch'\n",
    "_metricsTrain    = 'Resulting metrics (training)'\n",
    "_metricsTest     = 'Resulting metrics (testing)'\n",
    "\n",
    "_metricsPrint = [_accuracy, _recall, _precision, _f1, _auc]\n",
    "\n",
    "# Get a clean dictionary for the metrics.\n",
    "def getMetricsDict():\n",
    "    return {\n",
    "        _loss          : torch.tensor(0.),\n",
    "        _groundtruth   : torch.tensor([]),\n",
    "        _logits        : torch.tensor([])\n",
    "    }\n",
    "\n",
    "# Function used to update the dictionary of resulting metrics.\n",
    "def updateRunningMetrics(logits, groundtruth, loss, batchAmount, metricsResults):\n",
    "    # Accumulate the loss.\n",
    "    metricsResults[_loss] += loss.cpu() / batchAmount\n",
    "    # Accumulate the groundtruth and the logits.\n",
    "    metricsResults[_groundtruth] = torch.cat((metricsResults[_groundtruth], groundtruth.cpu())) \n",
    "    metricsResults[_logits] = torch.cat((metricsResults[_logits], logits.cpu()))\n",
    "\n",
    "# Function used to process the dictionary of resulting metrics (make final calculations).\n",
    "def processRunningMetrics(metricsResults):\n",
    "    # Detach the other values in the dictionary.\n",
    "    metricsResults[_loss] = metricsResults[_loss].detach()\n",
    "    metricsResults[_groundtruth] = metricsResults[_groundtruth].detach()\n",
    "    metricsResults[_logits] = metricsResults[_logits].detach()\n",
    "    # Save in the dictionary the probabilities and the predictions.\n",
    "    metricsResults[_probabilities] = softmax(metricsResults[_logits]).detach()\n",
    "    metricsResults[_predictions] = torch.argmax(metricsResults[_probabilities], axis=1).detach()\n",
    "\n",
    "    # Get Groundtruth (as numpy).\n",
    "    groundtruth = metricsResults[_groundtruth].detach().numpy()\n",
    "    # Get probabilities (as numpy).\n",
    "    probs = metricsResults[_probabilities].detach().numpy()\n",
    "    # Get predictions (as numpy).\n",
    "    preds = metricsResults[_predictions].detach().numpy()\n",
    "\n",
    "    # Calculate accuracy by class.\n",
    "    confusionMatrix = confusion_matrix(groundtruth, preds)\n",
    "    confusionMatrix = confusionMatrix.astype('float') / confusionMatrix.sum(axis=1)[:, np.newaxis]\n",
    "    metricsResults[_accuracyClass] = torch.tensor(confusionMatrix.diagonal())\n",
    "\n",
    "    # Calculate accuracy.\n",
    "    metricsResults[_accuracy] = torch.tensor(accuracy_score(groundtruth, preds))\n",
    "    # Calculate recall.\n",
    "    metricsResults[_recall] = torch.tensor(recall_score(groundtruth, preds, average='macro', zero_division=0))\n",
    "    # Calculate precision.\n",
    "    metricsResults[_precision] = torch.tensor(precision_score(groundtruth, preds, average='macro', zero_division=0))\n",
    "    # Calculate F1.\n",
    "    metricsResults[_f1] = torch.tensor(f1_score(groundtruth, preds, average='macro', zero_division=0))\n",
    "    # Calculate AUC.\n",
    "    metricsResults[_auc] = torch.tensor(roc_auc_score(groundtruth, probs, multi_class='ovr'))\n",
    "\n",
    "# Pretty print the metrics dictionaries.\n",
    "def printMetricsDict(metricsResults):\n",
    "    # All metrics to print\n",
    "    metricPrints = []\n",
    "\n",
    "    # Format the loss.\n",
    "    lossPrint = 'Loss: {:.4f}'.format(metricsResults[_loss])\n",
    "    metricPrints.append(lossPrint)\n",
    "\n",
    "    # Format the the remaining metrics.\n",
    "    baseMetricString = '{}: {:1.4f}'\n",
    "    for metric in _metricsPrint:\n",
    "        metricPrints.append(baseMetricString.format(metric, metricsResults[metric]))\n",
    "\n",
    "    print(', '.join(metricPrints))\n",
    "\n",
    "# This functions process an metrics result dictionary for wandb. Is necessary to indicte\n",
    "#   the metrics origin, training or testing.\n",
    "def processMetricsWandb(metricsResults, training=False):\n",
    "    # Get the prefix to log on wandb, the keys must be different.\n",
    "    resultsType = 'training' if training else 'testing'\n",
    "\n",
    "    _confusionMatrix = 'Confusion matrix'\n",
    "\n",
    "    # All the wandb keys are based in the original metrics results keys.\n",
    "    lossKey = '{} ({})'.format(_loss, resultsType)\n",
    "    metricsKeys = ['{} ({})'.format(_metric, resultsType) for _metric in _metricsPrint]\n",
    "    accuracyClassKeys = ['{} accuracy ({})'.format(_class, resultsType) for _class in config[classes]]\n",
    "    confusionMatrixKey = '{} ({})'.format(_confusionMatrix, resultsType)\n",
    "\n",
    "    # Get the confusion matrix\n",
    "    confusionMatrix = wandb.plot.confusion_matrix(y_true=metricsResults[_groundtruth].numpy(),\n",
    "        preds=metricsResults[_predictions].numpy(), class_names=config[classes], title=confusionMatrixKey)\n",
    "\n",
    "    # Make the dictionary for wandb and store the values.\n",
    "    wandbDict = {\n",
    "        lossKey            : metricsResults[_loss].item(),\n",
    "        confusionMatrixKey : confusionMatrix\n",
    "    }\n",
    "    for i in range(len(metricsKeys)):\n",
    "        wandbDict[metricsKeys[i]] = metricsResults[_metricsPrint[i]].item()\n",
    "    for i in range(config[classesLen]):\n",
    "        wandbDict[accuracyClassKeys[i]] = metricsResults[_accuracyClass][i].item()\n",
    "\n",
    "    # Return, to log later.\n",
    "    return wandbDict\n",
    "\n",
    "# Get the metrics dictionaries for wandb and log them.\n",
    "def logMetricsWandb(trainMetricsResults, testMetricsResults):\n",
    "    # Get both dictionaries for wandb.\n",
    "    wandbTrainDict = processMetricsWandb(trainMetricsResults, training=True)\n",
    "    wandbTestDict  = processMetricsWandb(testMetricsResults, training=False)\n",
    "\n",
    "    # Merge the dictionaries.\n",
    "    wandbDict = {**wandbTrainDict, **wandbTestDict}\n",
    "\n",
    "    # Log on wandb\n",
    "    wandb.log(wandbDict)\n",
    "\n",
    "# Function used to save the model and the metrics.\n",
    "def saveEpochData(trainMetricsResults, testMetricsResults, model, optimizer, epoch, rootPath):\n",
    "    # Create a dir for the current epoch.\n",
    "    runDir = os.path.join(os.getcwd(), rootPath, str(epoch))\n",
    "    Path(runDir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Path\n",
    "    savePath = os.path.join(runDir, 'model.pth')\n",
    "\n",
    "    # Make dict for torch.save\n",
    "    saveDict = {\n",
    "        _model     : model.state_dict(),\n",
    "        _optimizer : optimizer.state_dict(),\n",
    "        _epoch     : epoch\n",
    "    }\n",
    "\n",
    "    # Save both metrics, for train and test.\n",
    "    metricsResults = {\n",
    "        _metricsTrain : trainMetricsResults,\n",
    "        _metricsTest  : testMetricsResults\n",
    "    }\n",
    "\n",
    "    # Merge the save dict with the metricsResults dict.\n",
    "    saveDict = {**metricsResults, **saveDict}\n",
    "\n",
    "    # Save\n",
    "    torch.save(saveDict, savePath)"
   ]
  },
  {
   "source": [
    "### Loader function.\n",
    "Should return the training loader and test loader, a iterable object by batches."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation definitions.\n",
    "transformTrain = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(config[inputSize]),  # This one does a resize (it cuts randomly, it doesn't keep the whole image).\n",
    "        transforms.RandomHorizontalFlip(),                # Flip the image horizontally randomly.\n",
    "        transforms.ToTensor(),                            # Make the image a tensor.\n",
    "        transforms.Normalize(\n",
    "            [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Expected normalization for pretrained pytorch models.\n",
    "    ])\n",
    "transformTest = transforms.Compose([\n",
    "        transforms.Resize(config[inputSize]),             # Resize the image, keeping all pixels.\n",
    "        transforms.CenterCrop(config[inputSize]),         # Cut the image in the center.\n",
    "        transforms.ToTensor(),                            # Make the image a tensor.\n",
    "        transforms.Normalize(\n",
    "            [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # Expected normalization for pretrained pytorch models.\n",
    "    ])\n",
    "\n",
    "# Function used to get the data loaders.\n",
    "# A folder with two folders inside called train and test is expected as a rootPath.\n",
    "def getLoaders(rootPath):\n",
    "    trainPath = os.path.join(rootPath, 'labelTrain')\n",
    "    testPath  = os.path.join(rootPath, 'labelTest')\n",
    "\n",
    "    # Get the training and test data, apply the transformations.\n",
    "    trainset = torchvision.datasets.ImageFolder(root=trainPath, transform=transformTrain)\n",
    "    testset  = torchvision.datasets.ImageFolder(root=testPath,  transform=transformTest)\n",
    "\n",
    "    # Get the loaders, to iterate the data through batches.\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=config[batchSize], shuffle=True, num_workers=2)\n",
    "    testloader  = torch.utils.data.DataLoader(testset,  batch_size=config[batchSize], shuffle=True, num_workers=2)\n",
    "    \n",
    "    return trainloader, testloader"
   ]
  },
  {
   "source": [
    "### Training method.\n",
    "This method takes care of a single training pass. Another function call this one multiple times."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEpoch(dataloader, model, criterion, optimizer):\n",
    "    # Metrics for training.\n",
    "    metricsResults = getMetricsDict()\n",
    "\n",
    "    # Enable the grad, for training.\n",
    "    with torch.set_grad_enabled(True):\n",
    "\n",
    "        # Indicate that the model is going to be trained.\n",
    "        model.train()\n",
    "\n",
    "        # Loader len, for metrics calculation.\n",
    "        loaderLen = len(dataloader)\n",
    "\n",
    "        # Iterate the batches for training.\n",
    "        for batch in dataloader:\n",
    "            # Train the model.\n",
    "            # Get the inputs and labels, and move them to the selected device.\n",
    "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "            # Zero the gradient parameters.\n",
    "            optimizer.zero_grad()\n",
    "            # Get the predictions.\n",
    "            outputs = model(inputs)\n",
    "            # Calculate the error.\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Calculates the derivatives of the parameters that have a gradient.\n",
    "            loss.backward()\n",
    "            # Update the parameters based on the computer gradient.\n",
    "            optimizer.step()\n",
    "            # Metrics for the training set.\n",
    "            updateRunningMetrics(outputs, labels, loss, loaderLen, metricsResults)\n",
    "\n",
    "    return metricsResults"
   ]
  },
  {
   "source": [
    "### Evaluation method.\n",
    "This method evaluates the model for a specified dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, criterion):\n",
    "    # Metrics for testing.\n",
    "    metricsResults = getMetricsDict()\n",
    "\n",
    "    # Enable the grad, for training.\n",
    "    with torch.set_grad_enabled(False):\n",
    "\n",
    "        # Indicate that the model is going to be evaluated.\n",
    "        model.eval()\n",
    "\n",
    "        # Loader len, for metrics calculation.\n",
    "        loaderLen = len(dataloader)\n",
    "\n",
    "        # Iterate the batches for testing.\n",
    "        for batch in dataloader:\n",
    "            # Test the model.\n",
    "            # Get the inputs and labels, and move them to the selected device.\n",
    "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "            # Get the predictions.\n",
    "            outputs = model(inputs)\n",
    "            # Calculate the error.\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Metrics for the testing set.\n",
    "            updateRunningMetrics(outputs, labels, loss, loaderLen, metricsResults)\n",
    "\n",
    "    return metricsResults"
   ]
  },
  {
   "source": [
    "### Trainining and evaluate method.\n",
    "For the specific purpose of this project, in each epoch we evaluate metrics for each data set (training and testing) in each epoch, this method simplifies the process. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndEvaluate(trainloader, testloader, model, criterion, optimizer, savePath):\n",
    "\n",
    "    startTimeTotal = time.time()\n",
    "\n",
    "    for epoch in range(1, config[epochs] + 1):\n",
    "        \n",
    "        # Train the model.\n",
    "        trainMetricsResults = trainEpoch(trainloader, model, criterion, optimizer)\n",
    "        processRunningMetrics(trainMetricsResults)\n",
    "\n",
    "        # Evaluate the model.\n",
    "        testMetricsResults = evaluate(testloader, model, criterion)\n",
    "        processRunningMetrics(testMetricsResults)\n",
    "\n",
    "        # Log on wandb\n",
    "        logMetricsWandb(trainMetricsResults, testMetricsResults)\n",
    "\n",
    "        # Save model and metrics for the epochs.\n",
    "        saveEpochData(trainMetricsResults, testMetricsResults, model, optimizer, epoch, savePath)\n",
    "\n",
    "        # Print the results.\n",
    "        if epoch % 5 == 0:\n",
    "            print('**', '[', 'Epoch ', epoch, ']', '*' * 48, sep='')\n",
    "            print('\\tTraining results:', end=' ')\n",
    "            printMetricsDict(trainMetricsResults)\n",
    "            print('\\t Testing results:', end=' ')\n",
    "            printMetricsDict(testMetricsResults)\n",
    "        \n",
    "    # Print time\n",
    "    print('Epochs terminados')\n",
    "    print(\"--- %s seconds ---\" % (time.time() - startTimeTotal))"
   ]
  },
  {
   "source": [
    "### General method for execution."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeTest(net, dataPath, runName, savePath):\n",
    "    # Get criterion and optimizer.\n",
    "    # Optimizer and the loss funtion used to train the model.\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adagrad(net.parameters(), lr=config[learningRate])\n",
    "\n",
    "    # Get the loaders.\n",
    "    trainloader, testloader = getLoaders(dataPath)\n",
    "\n",
    "    # Init wandb\n",
    "    run = wandb.init(project='Classifier-UNET-RESNET', entity='tecai', config=config, name=runName)\n",
    "\n",
    "    # Train and evaluate\n",
    "    trainAndEvaluate(trainloader, testloader, net, criterion, optimizer, savePath)\n",
    "\n",
    "    # Finish wandb\n",
    "    run.finish()"
   ]
  },
  {
   "source": [
    "# Experiments\n",
    "The experiments seek to explore the transfer learning, so it will focus on seeing the performance of the model in different datasets, exchanging parameters learned from other runs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### First run\n",
    "For the first run we are going to explore the performance of resnet34 with pretrained parameters on the raw dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model.\n",
    "# Get a predefined model from pytorch, without the pretrained parameters.\n",
    "net = models.resnet34(pretrained=False)\n",
    "# Get the input size of the las layer of the model.\n",
    "llInputSize = net.fc.in_features\n",
    "# Modify the last layer of the model, to classify the amount of required classes.\n",
    "net.fc = nn.Linear(llInputSize, config[outputSize])\n",
    "# Load the model to the selected device.\n",
    "net.to(device)\n",
    "\n",
    "# Paths of data.\n",
    "dataPath = 'data/corrida1'\n",
    "savePath = 'runs/cnn(corrida1)'\n",
    "\n",
    "# Run name (for wanbd).\n",
    "runName = 'CNN (corrida 1)'\n",
    "\n",
    "# Execute.\n",
    "executeTest(net, dataPath, runName, savePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model.\n",
    "# Get a predefined model from pytorch, without the pretrained parameters.\n",
    "net = models.resnet34(pretrained=False)\n",
    "# Get the input size of the las layer of the model.\n",
    "llInputSize = net.fc.in_features\n",
    "# Modify the last layer of the model, to classify the amount of required classes.\n",
    "net.fc = nn.Linear(llInputSize, config[outputSize])\n",
    "# Load the model to the selected device.\n",
    "net.to(device)\n",
    "\n",
    "# Paths of data.\n",
    "dataPath = 'data/corrida2'\n",
    "savePath = 'runs/cnn(corrida2)'\n",
    "\n",
    "# Run name (for wanbd).\n",
    "runName = 'CNN (corrida 2)'\n",
    "\n",
    "# Execute.\n",
    "executeTest(net, dataPath, runName, savePath)"
   ]
  }
 ]
}