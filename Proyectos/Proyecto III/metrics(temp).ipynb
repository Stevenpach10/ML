{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd009cf3d5909f3828bf53c46564821f879a9c8405e40e27fb09b54d745264e4a6d",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports.\n",
    "import os\n",
    "import time\n",
    "import wandb\n",
    "import random\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class names.\n",
    "classes = 'classes'\n",
    "# Number of classes to classify.\n",
    "classesLen = 'classesLen'\n",
    "\n",
    "config = {\n",
    "    classes : [\n",
    "        'Apple - Apple scab',\n",
    "        'Apple - Black rot',\n",
    "        'Apple - Cedar apple rust',\n",
    "        'Apple - Healthy'\n",
    "        'Background without leaves',\n",
    "        'Blueberry - Healthy',\n",
    "        'Cherry - healthy',\n",
    "        'Cherry - Powdery mildew',\n",
    "        'Corn - Cercospora',\n",
    "        'Corn - Common rust',\n",
    "        'Corn - Healthy',\n",
    "        'Corn - Northern Leaf Blight',\n",
    "        'Grape - Black rot',\n",
    "        'Grape - Esca ',\n",
    "        'Grape - Healthy',\n",
    "        'Grape - Leaf blight',\n",
    "        'Orange - Haunglongbing',\n",
    "        'Peach - Bacterial spot',\n",
    "        'Peach - healthy',\n",
    "        'Pepper bell - Bacterial spot',\n",
    "        'Pepper bell - healthy',\n",
    "        'Potato - Early blight',\n",
    "        'Potato - Healthy',\n",
    "        'Potato - Late blight',\n",
    "        'Raspberry - healthy',\n",
    "        'Soybean - Healthy',\n",
    "        'Squash - Powdery mildew',\n",
    "        'Strawberry - Healthy',\n",
    "        'Strawberry - Leaf scorch',\n",
    "        'Tomato - Bacterial spot',\n",
    "        'Tomato - Early blight',\n",
    "        'Tomato - Healthy',\n",
    "        'Tomato - Late blight',\n",
    "        'Tomato - Leaf Mold',\n",
    "        'Tomato - Septoria leaf spot',\n",
    "        'Tomato - Spider mites',\n",
    "        'Tomato - Target Spot',\n",
    "        'Tomato - Mosaic virus',\n",
    "        'Tomato - Yellow Leaf Curl Virus'\n",
    "    ],\n",
    "    classesLen : 39\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# About metrics.\n",
    "# Metric dictionary keys.\n",
    "# For preprocessing.\n",
    "_loss        = 'Loss'\n",
    "_groundtruth = 'Groundtruth'\n",
    "_logits      = 'Logits'\n",
    "# For postprocessing.\n",
    "_probabilities = 'Probabilities'\n",
    "_predictions   = 'Predictions'\n",
    "_accuracyClass = 'Accuracy class'\n",
    "_accuracy      = 'Accuracy'\n",
    "_recall        = 'Recall'\n",
    "_precision     = 'Precision'\n",
    "_f1            = 'F1'\n",
    "_auc           = 'AUC'\n",
    "\n",
    "_metricsPrint = [_accuracy, _recall, _precision, _f1, _auc]\n",
    "\n",
    "# Get a clean dictionary for the metrics.\n",
    "def getMetricsDict():\n",
    "    return {\n",
    "        _loss          : torch.tensor(0.),\n",
    "        _groundtruth   : torch.tensor([]),\n",
    "        _logits        : torch.tensor([])\n",
    "    }\n",
    "\n",
    "# Function used to update the dictionary of resulting metrics.\n",
    "def updateRunningMetrics(logits, groundtruth, loss, batchAmount, metricsResults):\n",
    "    # Accumulate the loss.\n",
    "    metricsResults[_loss] += loss.cpu() / batchAmount\n",
    "    # Accumulate the groundtruth and the logits.\n",
    "    metricsResults[_groundtruth] = torch.cat((metricsResults[_groundtruth], groundtruth.cpu())) \n",
    "    metricsResults[_logits] = torch.cat((metricsResults[_logits], logits.cpu()))\n",
    "\n",
    "# Function used to process the dictionary of resulting metrics (make final calculations).\n",
    "def processRunningMetrics(metricsResults):\n",
    "    # Detach the other values in the dictionary.\n",
    "    metricsResults[_loss] = metricsResults[_loss].detach()\n",
    "    metricsResults[_groundtruth] = metricsResults[_groundtruth].detach()\n",
    "    metricsResults[_logits] = metricsResults[_logits].detach()\n",
    "    # Save in the dictionary the probabilities and the predictions.\n",
    "    metricsResults[_probabilities] = softmax(metricsResults[_logits]).detach()\n",
    "    metricsResults[_predictions] = torch.argmax(metricsResults[_probabilities], axis=1).detach()\n",
    "\n",
    "    # Get Groundtruth (as numpy).\n",
    "    groundtruth = metricsResults[_groundtruth].detach().numpy()\n",
    "    # Get probabilities (as numpy).\n",
    "    probs = metricsResults[_probabilities].detach().numpy()\n",
    "    # Get predictions (as numpy).\n",
    "    preds = metricsResults[_predictions].detach().numpy()\n",
    "\n",
    "    # Calculate accuracy by class.\n",
    "    confusionMatrix = confusion_matrix(groundtruth, preds)\n",
    "    confusionMatrix = confusionMatrix.astype('float') / confusionMatrix.sum(axis=1)[:, np.newaxis]\n",
    "    metricsResults[_accuracyClass] = torch.tensor(confusionMatrix.diagonal())\n",
    "\n",
    "    # Calculate accuracy.\n",
    "    metricsResults[_accuracy] = torch.tensor(accuracy_score(groundtruth, preds))\n",
    "    # Calculate recall.\n",
    "    metricsResults[_recall] = torch.tensor(recall_score(groundtruth, preds, average='macro'))\n",
    "    # Calculate precision.\n",
    "    metricsResults[_precision] = torch.tensor(precision_score(groundtruth, preds, average='macro'))\n",
    "    # Calculate F1.\n",
    "    metricsResults[_f1] = torch.tensor(f1_score(groundtruth, preds, average='macro'))\n",
    "    # Calculate AUC.\n",
    "    metricsResults[_auc] = torch.tensor(roc_auc_score(groundtruth, probs, multi_class='ovr'))\n",
    "\n",
    "# Pretty print the metrics dictionaries.\n",
    "def printMetricsDict(metricsResults):\n",
    "    # All metrics to print\n",
    "    metricPrints = []\n",
    "\n",
    "    # Format the loss.\n",
    "    lossPrint = 'Loss: {:.4f}'.format(metricsResults[_loss])\n",
    "    metricPrints.append(lossPrint)\n",
    "\n",
    "    # Format the the remaining metrics.\n",
    "    baseMetricString = '{}: {:1.4f}'\n",
    "    for metric in _metricsPrint:\n",
    "        metricPrints.append(baseMetricString.format(metric, metricsResults[metric]))\n",
    "\n",
    "    print(', '.join(metricPrints))\n",
    "\n",
    "# This functions process an metrics result dictionary for wandb. Is necessary to indicte\n",
    "#   the metrics origin, training or testing.\n",
    "def processMetricsWandb(metricsResults, training=False):\n",
    "    # Get the prefix to log on wandb, the keys must be different.\n",
    "    resultsType = 'training' if training else 'testing'\n",
    "\n",
    "    # Key name for the confusion matrix\n",
    "    _confusionMatrix = 'Confusion matrix'\n",
    "\n",
    "    # All the wandb keys are based in the original metrics results keys.\n",
    "    lossKey = '{} ({})'.format(_loss, resultsType)\n",
    "    metricsKeys = ['{} ({})'.format(_metric, resultsType) for _metric in _metricsPrint]\n",
    "    accuracyClassKeys = ['{} accuracy ({})'.format(_class, resultsType) for _class in config[classes]]\n",
    "    confusionMatrixKey = '{} ({})'.format(_confusionMatrix, resultsType)\n",
    "\n",
    "    # Get the confusion matrix\n",
    "    confusionMatrix = wandb.plot.confusion_matrix(y_true=metricsResults[_groundtruth].tolist(),\n",
    "        preds=metricsResults[_predictions].tolist(), class_names=config[classes], title=confusionMatrixKey)\n",
    "\n",
    "    # Make the dictionary for wandb and store the values.\n",
    "    wandbDict = {\n",
    "        lossKey            : metricsResults[_loss].item(),\n",
    "        confusionMatrixKey : confusionMatrix\n",
    "    }\n",
    "    for i in range(len(metricsKeys)):\n",
    "        wandbDict[metricsKeys[i]] = metricsResults[_metricsPrint[i]].item()\n",
    "\n",
    "    for i in range(config[classesLen]):\n",
    "        wandbDict[accuracyClassKeys[i]] = metricsResults[_accuracyClass][i].item()\n",
    "\n",
    "    # Return, to log later.\n",
    "    return wandbDict\n",
    "\n",
    "# Get the metrics dictionaries for wandb and log them.\n",
    "def logMetricsWandb(trainMetricsResults, testMetricsResults):\n",
    "    # Get both dictionaries for wandb.\n",
    "    wandbTrainDict = processMetricsWandb(trainMetricsResults, training=True)\n",
    "    wandbTestDict  = processMetricsWandb(testMetricsResults, training=False)\n",
    "\n",
    "    # Merge the dictionaries.\n",
    "    wandbDict = {**wandbTrainDict, **wandbTestDict}\n",
    "\n",
    "    # Log on wandb\n",
    "    wandb.log(wandbDict)\n",
    "\n",
    "# Function used to save the model and the metrics.\n",
    "def saveEpochData(trainMetricsResults, testMetricsResults, model, optimizer, epoch, rootPath):\n",
    "    # Create a dir for the current epoch.\n",
    "    runDir = os.path.join(os.getcwd(), rootPath, str(epoch))\n",
    "    Path(runDir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Path\n",
    "    savePath = os.path.join(runDir, 'model.pth')\n",
    "\n",
    "    # Make dict for torch.save\n",
    "    saveDict = {\n",
    "        _model     : model.state_dict(),\n",
    "        _optimizer : optimizer.state_dict(),\n",
    "        _epoch     : epoch\n",
    "    }\n",
    "\n",
    "    # Save both metrics, for train and test.\n",
    "    metricsResults = {\n",
    "        _metricsTrain : trainMetricsResults,\n",
    "        _metricsTest  : testMetricsResults\n",
    "    }\n",
    "\n",
    "    # Merge the save dict with the metricsResults dict.\n",
    "    saveDict = {**metricsResults, **saveDict}\n",
    "\n",
    "    # Save\n",
    "    torch.save(saveDict, savePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training**\n",
    "# Antes de cada epoch, obtengo un nuevo diccionario**\n",
    "# Es uno por training y uno por testing owo**\n",
    "trainMetricsResults = getMetricsDict()\n",
    "\n",
    "# Esto es el for del batch, lo q se hace en cada epoch\n",
    "for __groundtruth, __logits in zip(groundtruthTest, logitsTest):\n",
    "\n",
    "    # Dentro del epoch llamo a esta, le paso logits, grountruth, loss, len del epoch y el dictionario.\n",
    "    updateRunningMetrics(__logits, __groundtruth, lossTest, splitsAmount, trainMetricsResults)\n",
    "\n",
    "# Cuando termina el epoch llamo a esta para terminar de armar el dictionary\n",
    "processRunningMetrics(trainMetricsResults)\n",
    "\n",
    "# Testing**\n",
    "testMetricsResults = getMetricsDict()\n",
    "\n",
    "for __groundtruth, __logits in zip(groundtruthTest, logitsTest):\n",
    "\n",
    "    updateRunningMetrics(__logits, __groundtruth, lossTest, splitsAmount, testMetricsResults)\n",
    "\n",
    "processRunningMetrics(testMetricsResults)\n",
    "\n",
    "# Cuando tengo los dos diccionarios de resultados logeo en wandb**\n",
    "logMetricsWandb(trainMetricsResults, testMetricsResults)\n",
    "\n",
    "# Tambien puedo imprimirlos**\n",
    "epoch = 1\n",
    "print('**', '[', 'Epoch ', epoch, ']', '*' * 48, sep='')\n",
    "print('\\tTraining results:', end=' ')\n",
    "printMetricsDict(trainMetricsResults)\n",
    "print('\\t Testing results:', end=' ')\n",
    "printMetricsDict(testMetricsResults)\n",
    "\n",
    "# Tambien se puede guardar si quieren salvar el modelo y las metricas hasta el momento\n",
    "rootPath = 'runs/corrida1'\n",
    "saveEpochData(trainMetricsResults, testMetricsResults, model, optimizer, epoch, rootPath)"
   ]
  }
 ]
}