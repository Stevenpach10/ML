{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unet - YT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python395jvsc74a57bd01119e6db3b611a071a02fedbd618e93718260f90f2b64402566f1a15d771d1ec",
      "display_name": "Python 3.9.5 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.5"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7cd0bbe49f3245c19a15cf64505d8cf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_20fe327f50cf42d1ab97305299bc28a9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2c3271f3ff9942af846ccbbff043904d",
              "IPY_MODEL_a0aea6f466fa42b591092b413195048a"
            ]
          }
        },
        "20fe327f50cf42d1ab97305299bc28a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2c3271f3ff9942af846ccbbff043904d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_435a3ad70eec45928eaf0b41ec31bda1",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 71,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_758ef309ee1847ec91f356abf1590f97"
          }
        },
        "a0aea6f466fa42b591092b413195048a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_290c9a14d11443cfad8a6414eda7ea1b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/71 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_36ed32c60df24d3b86f6798cc1077158"
          }
        },
        "435a3ad70eec45928eaf0b41ec31bda1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "758ef309ee1847ec91f356abf1590f97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "290c9a14d11443cfad8a6414eda7ea1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "36ed32c60df24d3b86f6798cc1077158": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "metadata": {
      "interpreter": {
        "hash": "1119e6db3b611a071a02fedbd618e93718260f90f2b64402566f1a15d771d1ec"
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjjEEOiSfz-q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eab935fe-4ea0-4068-a88f-005d5da5174f"
      },
      "source": [
        "#!pip install albumentations==0.4.6"
      ],
      "execution_count": 300,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFC45biMuQdl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f8cc096-98db-460c-a0bb-fec5c2504302"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": 301,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Or1HS_RH4dbg"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "#augmentation \n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm.auto import tqdm\n",
        "import torch.optim as optim\n",
        "\n",
        "from numpy import float32\n",
        "\n"
      ],
      "execution_count": 302,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTmcR3-clod9"
      },
      "source": [
        "#Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh-YjvMVlsjy"
      },
      "source": [
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "  print(\"=> Saving checkpoint\")\n",
        "  torch.save(state, filename)\n",
        "\n",
        "def load_checkpoint(checkpoint, model):\n",
        "  print(\"=> Loading checkpoint\")\n",
        "  model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "def get_loaders(\n",
        "    train_dir,\n",
        "    train_maskdir,\n",
        "    val_dir,\n",
        "    val_maskdir,\n",
        "    batch_size,\n",
        "    train_transform,\n",
        "    val_transform,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "):\n",
        "  train_ds = CarvanaDataset(\n",
        "      image_dir = train_dir,\n",
        "      mask_dir  = train_maskdir,\n",
        "      transform = train_transform, \n",
        "  )\n",
        "  train_loader = DataLoader(\n",
        "      train_ds,\n",
        "      batch_size=batch_size,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=pin_memory,\n",
        "      shuffle=True,\n",
        "  )\n",
        "\n",
        "  val_ds = CarvanaDataset(\n",
        "      image_dir = val_dir,\n",
        "      mask_dir  = val_maskdir,\n",
        "      transform = val_transform, \n",
        "  )\n",
        "  val_loader = DataLoader(\n",
        "      val_ds,\n",
        "      batch_size=batch_size,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=pin_memory,\n",
        "      shuffle=False,\n",
        "  )\n",
        "\n",
        "  return train_loader, val_loader\n",
        "\n",
        "def check_accuracy(loader, model, device=\"cuda\"):\n",
        "  num_correct = 0\n",
        "  num_pixels = 0\n",
        "  #Better metric than accuracy\n",
        "  dice_score = 0\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x, y in loader:\n",
        "      x = x.to(device)\n",
        "      #The label doesnt have a channel because is a gray scale\n",
        "    \n",
        "      #The model returns the logits, so we need to use an activation function\n",
        "      if NUM_CLASSES > 1:\n",
        "         y = y.to(device)\n",
        "         preds = model(x)\n",
        "      else:\n",
        "        y = y.to(device).unsqueeze(1)\n",
        "        preds = torch.sigmoid(model(x))\n",
        "        preds = (preds > 0.5).float()\n",
        "        num_correct += (preds == y).sum()\n",
        "        num_pixels = torch.numel(preds)\n",
        "\n",
        "      dice_score += (2 * (preds * y).sum() / ((preds + y).sum() + 1e-8) )\n",
        "  \n",
        "  print(f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.3f}\")\n",
        "  print(f\"Dice score: {dice_score/len(loader)}\")\n",
        "  model.train()\n",
        "\n",
        "def save_predictions_as_imgs(loader, model, folder=\"saved_images/\", device=\"cuda\"):\n",
        "  model.eval()\n",
        "  for idx, (x, y) in enumerate(loader):\n",
        "    x = x.to(device=device)\n",
        "    with torch.no_grad():\n",
        "      preds = model(x)\n",
        "      torchvision.utils.save_image(preds, f\"{folder}/pred_{idx}.png\")\n",
        "      if NUM_CLASSES > 1:\n",
        "        torchvision.utils.save_image(y, f\"{folder}/y_{idx}.png\")\n",
        "      else:\n",
        "        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}/y_{idx}.png\")\n",
        "      \n",
        "      \n",
        "  model.train()"
      ],
      "execution_count": 303,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObeunEdvLbxm"
      },
      "source": [
        "# Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW9mJFvU42Mc"
      },
      "source": [
        "class DoubleConv(nn.Module):\n",
        "  def __init__ (self, in_channels, out_channels):\n",
        "    super(DoubleConv, self).__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "        #Here we add Batch Normalization to improve the paper's model\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)\n",
        "      "
      ],
      "execution_count": 304,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5pv6eDf61RC"
      },
      "source": [
        "class UNET(nn.Module):\n",
        "  #features represent the original paper dimensions.\n",
        "  def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n",
        "    super(UNET, self).__init__()\n",
        "    #Encoder part\n",
        "    self.downs = nn.ModuleList()\n",
        "    #Decoder part\n",
        "    self.ups = nn.ModuleList()\n",
        "    #Pool\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "   #Create a list of contracting path\n",
        "    for feature in features:\n",
        "      self.downs.append(DoubleConv(in_channels, feature))\n",
        "      in_channels = feature\n",
        "\n",
        "   #Create a list of expansive path\n",
        "    for feature in reversed(features):\n",
        "      self.ups.append(\n",
        "         #Featur2 * 2 is to create the 1024 dim\n",
        "         nn.ConvTranspose2d(feature *2, feature, kernel_size=2, stride=2,)\n",
        "     )\n",
        "      self.ups.append(DoubleConv(feature * 2, feature))\n",
        "  \n",
        "    self.bottleneck = DoubleConv(features[-1], features[-1] * 2)\n",
        "    self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "   \n",
        "    #For save the connections with the up part\n",
        "    skip_connections = []\n",
        "    for down in self.downs:\n",
        "      x = down(x)\n",
        "      skip_connections.append(x)\n",
        "      x = self.pool(x)\n",
        "    #Here is the bottom part of the net\n",
        "    x = self.bottleneck(x)\n",
        "    \n",
        "    #Start the up part\n",
        "    #Reverse list\n",
        "    skip_connections = skip_connections[:: -1]\n",
        "\n",
        "    #Step of two because we use up and doubleconv\n",
        "    #0 is the up\n",
        "    #1 is the double conv\n",
        "    for idx in range (0, len(self.ups), 2):\n",
        "      x = self.ups[idx](x)\n",
        "      #Divide idx by 2 for going liner with the skip connections\n",
        "      skip_connection = skip_connections[idx//2]\n",
        "\n",
        "      #General solutions for image tha not are divisibles\n",
        "      if x.shape != skip_connection.shape:\n",
        "        #Take the H and W, skip the Batch Size and Channels\n",
        "        x = TF.resize(x, size=skip_connection.shape[2:])\n",
        "      #Add the skip connection\n",
        "      concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "      # (e.g 0 +1 for the doubleconv)\n",
        "      x = self.ups[idx+1](concat_skip)\n",
        "    \n",
        "    return self.final_conv(x)"
      ],
      "execution_count": 305,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UlkVDxdFK1Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57f01ec7-6eac-4d5d-dd4c-9d047b5e8639"
      },
      "source": [
        "def test():\n",
        "  #Batch, Channel, H, W\n",
        "  x = torch.randn((3,3,160,160))\n",
        "  model = UNET(in_channels=3,out_channels=3)\n",
        "  preds = model(x)\n",
        "  print(preds.shape)\n",
        "  print(x.shape)\n",
        "\n",
        "  assert preds.shape == x.shape\n",
        "\n",
        "test()"
      ],
      "execution_count": 306,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 3, 160, 160])\ntorch.Size([3, 3, 160, 160])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8ce8Pz5LfyA"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UsuzSPgLfTm"
      },
      "source": [
        "class CarvanaDataset(Dataset):\n",
        "  def __init__(self, image_dir, mask_dir, transform=None):\n",
        "    self.image_dir = image_dir\n",
        "    self.mask_dir = mask_dir\n",
        "    self.transform = transform\n",
        "    self.images = os.listdir(self.image_dir)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "     img_path = os.path.join(self.image_dir, self.images[index])\n",
        "     mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".jpg\", \"_mask.gif\"))\n",
        "     \n",
        "     #The image input is an RBG but the image mask is in a grayscale\n",
        "     image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "     if NUM_CLASSES > 1:\n",
        "       mask = np.array(Image.open(mask_path).convert(\"RGB\"))\n",
        "     else:\n",
        "       mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=float32)\n",
        "     \n",
        "     \n",
        "     #mask[mask == 255.0] = 1.0\n",
        "     if self.transform is not None:\n",
        "       augmentations = self.transform(image=image, mask=mask)\n",
        "       image = augmentations[\"image\"]\n",
        "       mask = augmentations[\"mask\"]\n",
        "     return image, mask"
      ],
      "execution_count": 307,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0xBi7m4V_yU"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJ0L2xyjVDqf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9d80d87-27a2-4fc9-aa47-7e0e4e0bb79c"
      },
      "source": [
        "LEARNING_RATE = 1e-4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 1\n",
        "NUM_EPOCHS = 5\n",
        "NUM_WORKERS = 0\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 3\n",
        "IMAGE_HEIGHT = 160 #Originally 1280\n",
        "IMAGE_WIDTH = 240 #Originally 1918\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = False\n",
        "#TRAIN_IMG_DIR = \"/content/drive/MyDrive/plants/train\"\n",
        "#TRAIN_MASK_DIR = \"/content/drive/MyDrive/plants/train_masks\"\n",
        "#VAL_IMG_DIR = \"/content/drive/MyDrive/plants/val\"\n",
        "#VAL_MASK_DIR = \"/content/drive/MyDrive/plants/val_masks\"\n",
        "\n",
        "TRAIN_IMG_DIR = \"G:/Mi unidad/plants/train\"\n",
        "TRAIN_MASK_DIR = \"G:/Mi unidad/plants/train_masks\"\n",
        "VAL_IMG_DIR = \"G:/Mi unidad/plants/val\"\n",
        "VAL_MASK_DIR = \"G:/Mi unidad/plants/val_masks\"\n",
        "\n",
        "#TRAIN_IMG_DIR = \"G:/Mi unidad/cavana/train\"\n",
        "#TRAIN_MASK_DIR = \"G:/Mi unidad/cavana/train_masks\"\n",
        "#VAL_IMG_DIR = \"G:/Mi unidad/cavana/val\"\n",
        "#VAL_MASK_DIR = \"G:/Mi unidad/cavana/val_masks\"\n",
        "\n",
        "print(DEVICE)"
      ],
      "execution_count": 308,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxNzAZ2wXnb4"
      },
      "source": [
        "# Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BA4cjrWXmyX"
      },
      "source": [
        "#Will going to do 1 epoch training\n",
        "def train_fn(loader, model, optimizer, loss_fn, scaler):\n",
        "  #Progress bar\n",
        "  loop = tqdm(loader)\n",
        "\n",
        "  for batch_idx, (data, targets) in enumerate (loop):\n",
        "    data = data.to(device=DEVICE)\n",
        "    #For the Binary Cross Entropy using must be a float\n",
        "    #Unsqueese is for adding a channel dimension.\n",
        "    if NUM_CLASSES > 1:\n",
        "      targets = targets.float().to(device=DEVICE)\n",
        "    else:\n",
        "      targets = targets.float().unsqueeze(1).to(device=DEVICE)\n",
        "    \n",
        "\n",
        "    #Forward\n",
        "    #Run with mixture precision float 32 ops with float16 and stuffs like that.\n",
        "    with torch.cuda.amp.autocast():\n",
        "      if NUM_CLASSES > 1:\n",
        "        predictions = model(data)\n",
        "        predictions = predictions.float().to(device=DEVICE)\n",
        "        loss = loss_fn(predictions, targets)\n",
        "      else:\n",
        "        predictions = model(data)\n",
        "        predictions = predictions.float().to(device=DEVICE)\n",
        "        loss = loss_fn(predictions, targets)\n",
        "    \n",
        "    #Backwards\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer=optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    #update tqdm loop\n",
        "\n",
        "    loop.set_postfix(loss=loss.item())"
      ],
      "execution_count": 309,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ1cL2dDfXBw"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eABd8kHhrXJ"
      },
      "source": [
        "## Transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3E7rDBVfSnV"
      },
      "source": [
        "train_transform = A.Compose([\n",
        "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "    A.Rotate(limit=35, p=1.0),\n",
        "    A.HorizontalFlip(p=0.1),\n",
        "    A.Normalize(\n",
        "        mean=[0.0, 0.0, 0.0],\n",
        "        std=[1.0, 1.0, 1.0],\n",
        "        max_pixel_value=255.0,\n",
        "    ),\n",
        "    ToTensorV2(),\n",
        "    #TODO change if is a gray scale image\n",
        "],additional_targets={'image': 'image', 'mask': 'image'})\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "    A.Normalize(\n",
        "        mean=[0.0, 0.0, 0.0],\n",
        "        std=[1.0, 1.0, 1.0],\n",
        "        max_pixel_value=255.0,\n",
        "    ),\n",
        "    ToTensorV2(),\n",
        "    #TODO\n",
        "],additional_targets={'image': 'image', 'mask': 'image'})"
      ],
      "execution_count": 310,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ffklV-uht8f"
      },
      "source": [
        "## Create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE4-FwmThwQo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510,
          "referenced_widgets": [
            "7cd0bbe49f3245c19a15cf64505d8cf7",
            "20fe327f50cf42d1ab97305299bc28a9",
            "2c3271f3ff9942af846ccbbff043904d",
            "a0aea6f466fa42b591092b413195048a",
            "435a3ad70eec45928eaf0b41ec31bda1",
            "758ef309ee1847ec91f356abf1590f97",
            "290c9a14d11443cfad8a6414eda7ea1b",
            "36ed32c60df24d3b86f6798cc1077158"
          ]
        },
        "outputId": "3f738f55-70a7-48f5-feb5-d7364f830199",
        "tags": []
      },
      "source": [
        "model = UNET(in_channels=NUM_CHANNELS, out_channels=NUM_CLASSES).to(DEVICE)\n",
        "#With Logits because the model has not activation function\n",
        "if NUM_CLASSES > 1:\n",
        "  loss = nn.MSELoss()\n",
        "else:\n",
        "  loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "#Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "train_loader, val_loader = get_loaders(\n",
        "    TRAIN_IMG_DIR,\n",
        "    TRAIN_MASK_DIR,\n",
        "    VAL_IMG_DIR,\n",
        "    VAL_MASK_DIR,\n",
        "    BATCH_SIZE,\n",
        "    train_transform,\n",
        "    val_transform,\n",
        "    NUM_WORKERS,\n",
        "    PIN_MEMORY,\n",
        ")\n",
        "\n",
        "if LOAD_MODEL:\n",
        "        load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model)\n",
        "\n",
        "#check_accuracy(val_loader, model, device=DEVICE)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  train_fn(train_loader, model, optimizer, loss, scaler)\n",
        "\n",
        "  checkpoint = {\n",
        "      \"state_dict\"  : model.state_dict(),\n",
        "      \"optimizer\"   : optimizer.state_dict()\n",
        "  }\n",
        "\n",
        "  save_checkpoint(checkpoint)\n",
        "\n",
        "  #Check acc\n",
        "\n",
        "  #check_accuracy(val_loader, model, device=DEVICE)\n",
        "\n",
        "  save_predictions_as_imgs(val_loader, model, folder=\"saved_images\", device=DEVICE)\n",
        "\n",
        "#Loader\n",
        "\n"
      ],
      "execution_count": 311,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [01:13<00:00,  1.04s/it, loss=0.0159]\n",
            "=> Saving checkpoint\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [01:13<00:00,  1.03s/it, loss=0.00426]\n",
            "=> Saving checkpoint\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [01:14<00:00,  1.05s/it, loss=0.00308]\n",
            "=> Saving checkpoint\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [01:13<00:00,  1.03s/it, loss=0.00267]\n",
            "=> Saving checkpoint\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [01:12<00:00,  1.02s/it, loss=0.00227]\n",
            "=> Saving checkpoint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zorAnFwu3Ka4"
      },
      "source": [
        "x = torch.randn((1,160,180,3))\n",
        "print(x.shape)\n",
        "\n",
        "print(x.permute(0,3,1,2).shape)"
      ],
      "execution_count": 312,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 160, 180, 3])\ntorch.Size([1, 3, 160, 180])\n"
          ]
        }
      ]
    }
  ]
}