{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unet - YT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python395jvsc74a57bd01119e6db3b611a071a02fedbd618e93718260f90f2b64402566f1a15d771d1ec",
      "display_name": "Python 3.9.5 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.5"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7cd0bbe49f3245c19a15cf64505d8cf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_20fe327f50cf42d1ab97305299bc28a9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2c3271f3ff9942af846ccbbff043904d",
              "IPY_MODEL_a0aea6f466fa42b591092b413195048a"
            ]
          }
        },
        "20fe327f50cf42d1ab97305299bc28a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2c3271f3ff9942af846ccbbff043904d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_435a3ad70eec45928eaf0b41ec31bda1",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 71,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_758ef309ee1847ec91f356abf1590f97"
          }
        },
        "a0aea6f466fa42b591092b413195048a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_290c9a14d11443cfad8a6414eda7ea1b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/71 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_36ed32c60df24d3b86f6798cc1077158"
          }
        },
        "435a3ad70eec45928eaf0b41ec31bda1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "758ef309ee1847ec91f356abf1590f97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "290c9a14d11443cfad8a6414eda7ea1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "36ed32c60df24d3b86f6798cc1077158": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "metadata": {
      "interpreter": {
        "hash": "1119e6db3b611a071a02fedbd618e93718260f90f2b64402566f1a15d771d1ec"
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjjEEOiSfz-q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eab935fe-4ea0-4068-a88f-005d5da5174f"
      },
      "source": [
        "#!pip install albumentations==0.4.6"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFC45biMuQdl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f8cc096-98db-460c-a0bb-fec5c2504302"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Or1HS_RH4dbg"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "#augmentation \n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm.auto import tqdm\n",
        "import torch.optim as optim\n",
        "\n",
        "from numpy import float32\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTmcR3-clod9"
      },
      "source": [
        "#Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh-YjvMVlsjy"
      },
      "source": [
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "  print(\"=> Saving checkpoint\")\n",
        "  torch.save(state, filename)\n",
        "\n",
        "def load_checkpoint(checkpoint, model):\n",
        "  print(\"=> Loading checkpoint\")\n",
        "  model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "def get_loaders(\n",
        "    train_dir,\n",
        "    train_maskdir,\n",
        "    val_dir,\n",
        "    val_maskdir,\n",
        "    batch_size,\n",
        "    train_transform,\n",
        "    val_transform,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "):\n",
        "  train_ds = CarvanaDataset(\n",
        "      image_dir = train_dir,\n",
        "      mask_dir  = train_maskdir,\n",
        "      transform = train_transform, \n",
        "  )\n",
        "  train_loader = DataLoader(\n",
        "      train_ds,\n",
        "      batch_size=batch_size,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=pin_memory,\n",
        "      shuffle=True,\n",
        "  )\n",
        "\n",
        "  val_ds = CarvanaDataset(\n",
        "      image_dir = val_dir,\n",
        "      mask_dir  = val_maskdir,\n",
        "      transform = val_transform, \n",
        "  )\n",
        "  val_loader = DataLoader(\n",
        "      val_ds,\n",
        "      batch_size=batch_size,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=pin_memory,\n",
        "      shuffle=False,\n",
        "  )\n",
        "\n",
        "  return train_loader, val_loader\n",
        "\n",
        "#TODO Change the accuracy metric for RBG image.\n",
        "def check_accuracy(loader, model, device=\"cuda\"):\n",
        "  num_correct = 0\n",
        "  num_pixels = 0\n",
        "  #Better metric than accuracy\n",
        "  dice_score = 0\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x, y in loader:\n",
        "      x = x.to(device)\n",
        "      #The label doesnt have a channel because is a gray scale\n",
        "    \n",
        "      #The model returns the logits, so we need to use an activation function\n",
        "      if NUM_CLASSES > 1:\n",
        "         y = y.to(device)\n",
        "         preds = model(x)\n",
        "      else:\n",
        "        y = y.to(device).unsqueeze(1)\n",
        "        preds = torch.sigmoid(model(x))\n",
        "        preds = (preds > 0.5).float()\n",
        "        num_correct += (preds == y).sum()\n",
        "        num_pixels = torch.numel(preds)\n",
        "\n",
        "      dice_score += (2 * (preds * y).sum() / ((preds + y).sum() + 1e-8) )\n",
        "  \n",
        "  print(f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.3f}\")\n",
        "  print(f\"Dice score: {dice_score/len(loader)}\")\n",
        "  model.train()\n",
        "\n",
        "def save_predictions_as_imgs(loader, model, folder=\"saved_images/\", device=\"cuda\"):\n",
        "  model.eval()\n",
        "  for idx, (x, y) in enumerate(loader):\n",
        "    x = x.to(device=device)\n",
        "    with torch.no_grad():\n",
        "      preds = model(x)\n",
        "      torchvision.utils.save_image(preds, f\"{folder}/pred_{idx}.png\")\n",
        "      if NUM_CLASSES > 1:\n",
        "        torchvision.utils.save_image(y, f\"{folder}/y_{idx}.png\")\n",
        "      else:\n",
        "        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}/y_{idx}.png\")\n",
        "      \n",
        "      \n",
        "  model.train()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObeunEdvLbxm"
      },
      "source": [
        "# Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW9mJFvU42Mc"
      },
      "source": [
        "class DoubleConv(nn.Module):\n",
        "  def __init__ (self, in_channels, out_channels):\n",
        "    super(DoubleConv, self).__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "        #Here we add Batch Normalization to improve the paper's model\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)\n",
        "      "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5pv6eDf61RC"
      },
      "source": [
        "class UNET(nn.Module):\n",
        "  #features represent the original paper dimensions.\n",
        "  def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512, 1024], linearFeature=2048, isOnlyEncoder=False):\n",
        "    super(UNET, self).__init__()\n",
        "    #Encoder part\n",
        "    self.downs = nn.ModuleList()\n",
        "    #Decoder part\n",
        "    self.ups = nn.ModuleList()\n",
        "    #Pool\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.linearFeature = linearFeature\n",
        "    self.isOnlyEncoder = isOnlyEncoder\n",
        "    self.features = features\n",
        "\n",
        "   #Create a list of contracting path\n",
        "    for feature in features:\n",
        "      self.downs.append(DoubleConv(in_channels, feature))\n",
        "      in_channels = feature\n",
        "\n",
        "   #Create a list of expansive path\n",
        "    for feature in reversed(features):\n",
        "      self.ups.append(\n",
        "         #Featur2 * 2 is to create the 1024 dim\n",
        "         nn.ConvTranspose2d(feature *2, feature, kernel_size=2, stride=2,)\n",
        "     )\n",
        "      self.ups.append(DoubleConv(feature * 2, feature))\n",
        "  \n",
        "    self.bottleneck = DoubleConv(features[-1], features[-1]*2)#1024*10*10\n",
        "    print(\"Bottleneck\")\n",
        "    print(self.bottleneck.shape)\n",
        "    self.linear1 = nn.Linear(self.linearFeature*10*10, self.linearFeature)\n",
        "    self.linear2 = nn.Linear(self.linearFeature, self.linearFeature*10*10)\n",
        "\n",
        "    self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    \n",
        "\n",
        "  def forward(self, x):\n",
        "   \n",
        "    #For save the connections with the up part\n",
        "    skip_connections = []\n",
        "    for down in self.downs:\n",
        "      x = down(x)\n",
        "      skip_connections.append(x)\n",
        "      x = self.pool(x)\n",
        "    #Here is the bottom part of the net\n",
        "    x = self.bottleneck(x)\n",
        "    x = torch.flatten(x, start_dim=1)#102400 -> 51200 -> 1024 -> 51200 -> 102400 \n",
        "    x = self.linear1(x)\n",
        "\n",
        "    #Check if we are using only de encoder part\n",
        "    if self.isOnlyEncoder:\n",
        "      return x\n",
        "    x = self.linear2(x)\n",
        "    x = x.reshape((x.shape[0], self.linearFeature, 10, 10))\n",
        "    \n",
        "    \n",
        "    #Start the up part\n",
        "    #Reverse list\n",
        "    skip_connections = skip_connections[:: -1]\n",
        "\n",
        "    #Step of two because we use up and doubleconv\n",
        "    #0 is the up\n",
        "    #1 is the double conv\n",
        "    for idx in range (0, len(self.ups), 2):\n",
        "      x = self.ups[idx](x)\n",
        "      #Divide idx by 2 for going liner with the skip connections\n",
        "      skip_connection = skip_connections[idx//2]\n",
        "\n",
        "      #General solutions for image tha not are divisibles\n",
        "      if x.shape != skip_connection.shape:\n",
        "        #Take the H and W, skip the Batch Size and Channels\n",
        "        x = TF.resize(x, size=skip_connection.shape[2:])\n",
        "      #Add the skip connection\n",
        "      concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "      # (e.g 0 +1 for the doubleconv)\n",
        "      x = self.ups[idx+1](concat_skip)\n",
        "    \n",
        "    return self.final_conv(x)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UlkVDxdFK1Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57f01ec7-6eac-4d5d-dd4c-9d047b5e8639"
      },
      "source": [
        "def test():\n",
        "  #Batch, Channel, H, W\n",
        "  x = torch.randn((3,3,160,160))\n",
        "  model = UNET(in_channels=3,out_channels=3, isOnlyEncoder=True)\n",
        "  preds = model(x)\n",
        "  print(preds.shape)\n",
        "  print(x.shape)\n",
        "\n",
        "  #assert preds.shape == x.shape\n",
        "\n",
        "test()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 1024])\ntorch.Size([3, 3, 160, 160])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8ce8Pz5LfyA"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UsuzSPgLfTm"
      },
      "source": [
        "class CarvanaDataset(Dataset):\n",
        "  def __init__(self, image_dir, mask_dir, transform=None):\n",
        "    self.image_dir = image_dir\n",
        "    self.mask_dir = mask_dir\n",
        "    self.transform = transform\n",
        "    self.images = os.listdir(self.image_dir)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "     img_path = os.path.join(self.image_dir, self.images[index])\n",
        "     mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".jpg\", \"_mask.gif\"))\n",
        "     \n",
        "     #The image input is an RBG but the image mask is in a grayscale\n",
        "     image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "     if NUM_CLASSES > 1:\n",
        "       mask = np.array(Image.open(mask_path).convert(\"RGB\"))\n",
        "     else:\n",
        "       mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=float32)\n",
        "     \n",
        "     \n",
        "     #mask[mask == 255.0] = 1.0\n",
        "     if self.transform is not None:\n",
        "       augmentations = self.transform(image=image, mask=mask)\n",
        "       image = augmentations[\"image\"]\n",
        "       mask = augmentations[\"mask\"]\n",
        "     return image, mask"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0xBi7m4V_yU"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJ0L2xyjVDqf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9d80d87-27a2-4fc9-aa47-7e0e4e0bb79c"
      },
      "source": [
        "#TODO: DICC\n",
        "\n",
        "LEARNING_RATE = 1e-4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 5\n",
        "NUM_EPOCHS = 5\n",
        "NUM_WORKERS = 0\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 3\n",
        "IMAGE_HEIGHT = 160 #Originally 1280\n",
        "IMAGE_WIDTH = 160 #Originally 1918\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = False\n",
        "#TRAIN_IMG_DIR = \"/content/drive/MyDrive/plants/train\"\n",
        "#TRAIN_MASK_DIR = \"/content/drive/MyDrive/plants/train_masks\"\n",
        "#VAL_IMG_DIR = \"/content/drive/MyDrive/plants/val\"\n",
        "#VAL_MASK_DIR = \"/content/drive/MyDrive/plants/val_masks\"\n",
        "\n",
        "TRAIN_IMG_DIR = \"G:/Mi unidad/plants/train\"\n",
        "TRAIN_MASK_DIR = \"G:/Mi unidad/plants/train_masks\"\n",
        "VAL_IMG_DIR = \"G:/Mi unidad/plants/val\"\n",
        "VAL_MASK_DIR = \"G:/Mi unidad/plants/val_masks\"\n",
        "\n",
        "TRAIN_CLASS_IMG_DIR = \"G:/Mi unidad/plants/classifier_train\"\n",
        "VAL_CLASS_IMG_DIR = \"G:/Mi unidad/plants/classifier_train\"\n",
        "\n",
        "#TRAIN_IMG_DIR = \"G:/Mi unidad/cavana/train\"\n",
        "#TRAIN_MASK_DIR = \"G:/Mi unidad/cavana/train_masks\"\n",
        "#VAL_IMG_DIR = \"G:/Mi unidad/cavana/val\"\n",
        "#VAL_MASK_DIR = \"G:/Mi unidad/cavana/val_masks\"\n",
        "\n",
        "print(DEVICE)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxNzAZ2wXnb4"
      },
      "source": [
        "# Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BA4cjrWXmyX"
      },
      "source": [
        "#Do 1 epoch training\n",
        "def train_fn(loader, model, optimizer, loss_fn, scaler):\n",
        "  #Progress bar\n",
        "  loop = tqdm(loader)\n",
        "\n",
        "  for batch_idx, (data, targets) in enumerate (loop):\n",
        "    data = data.to(device=DEVICE)\n",
        "    #For the Binary Cross Entropy using must be a float\n",
        "    #Unsqueese is for adding a channel dimension.\n",
        "    if NUM_CLASSES > 1:\n",
        "      targets = targets.float().to(device=DEVICE)\n",
        "    else:\n",
        "      targets = targets.float().unsqueeze(1).to(device=DEVICE)\n",
        "    \n",
        "\n",
        "    #Forward\n",
        "    #Run with mixture precision float 32 ops with float16 and stuffs like that.\n",
        "    with torch.cuda.amp.autocast():\n",
        "      if NUM_CLASSES > 1:\n",
        "        predictions = model(data)\n",
        "        predictions = predictions.float().to(device=DEVICE)\n",
        "        loss = loss_fn(predictions, targets)\n",
        "      else:\n",
        "        predictions = model(data)\n",
        "        predictions = predictions.float().to(device=DEVICE)\n",
        "        loss = loss_fn(predictions, targets)\n",
        "    \n",
        "    #Backwards\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer=optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    #update tqdm loop\n",
        "\n",
        "    loop.set_postfix(loss=loss.item())"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ1cL2dDfXBw"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eABd8kHhrXJ"
      },
      "source": [
        "## Transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3E7rDBVfSnV"
      },
      "source": [
        "train_transform = A.Compose([\n",
        "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "    A.Rotate(limit=35, p=1.0),\n",
        "    A.HorizontalFlip(p=0.1),\n",
        "    A.Normalize(\n",
        "        mean=[0.0, 0.0, 0.0],\n",
        "        std=[1.0, 1.0, 1.0],\n",
        "        max_pixel_value=255.0,\n",
        "    ),\n",
        "    # B H W C\n",
        "    # B C H W \n",
        "    ToTensorV2(),\n",
        "    #TODO change if is a gray scale image\n",
        "],additional_targets={'image': 'image', 'mask': 'image'})\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "    A.Normalize(\n",
        "        mean=[0.0, 0.0, 0.0],\n",
        "        std=[1.0, 1.0, 1.0],\n",
        "        max_pixel_value=255.0,\n",
        "    ),\n",
        "    ToTensorV2(),\n",
        "    #TODO\n",
        "],additional_targets={'image': 'image', 'mask': 'image'})"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ffklV-uht8f"
      },
      "source": [
        "## Create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE4-FwmThwQo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510,
          "referenced_widgets": [
            "7cd0bbe49f3245c19a15cf64505d8cf7",
            "20fe327f50cf42d1ab97305299bc28a9",
            "2c3271f3ff9942af846ccbbff043904d",
            "a0aea6f466fa42b591092b413195048a",
            "435a3ad70eec45928eaf0b41ec31bda1",
            "758ef309ee1847ec91f356abf1590f97",
            "290c9a14d11443cfad8a6414eda7ea1b",
            "36ed32c60df24d3b86f6798cc1077158"
          ]
        },
        "outputId": "3f738f55-70a7-48f5-feb5-d7364f830199",
        "tags": []
      },
      "source": [
        "model = UNET(in_channels=NUM_CHANNELS, out_channels=NUM_CLASSES).to(DEVICE)\n",
        "#With Logits because the model has not activation function\n",
        "if NUM_CLASSES > 1:\n",
        "  loss = nn.MSELoss()\n",
        "else:\n",
        "  loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "#Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "train_loader, val_loader = get_loaders(\n",
        "    TRAIN_IMG_DIR,\n",
        "    TRAIN_MASK_DIR,\n",
        "    VAL_IMG_DIR,\n",
        "    VAL_MASK_DIR,\n",
        "    BATCH_SIZE,\n",
        "    train_transform,\n",
        "    val_transform,\n",
        "    NUM_WORKERS,\n",
        "    PIN_MEMORY,\n",
        ")\n",
        "\n",
        "if LOAD_MODEL:\n",
        "        load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model)\n",
        "\n",
        "#check_accuracy(val_loader, model, device=DEVICE)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  train_fn(train_loader, model, optimizer, loss, scaler)\n",
        "\n",
        "  checkpoint = {\n",
        "      \"state_dict\"  : model.state_dict(),\n",
        "      \"optimizer\"   : optimizer.state_dict()\n",
        "  }\n",
        "\n",
        "  save_checkpoint(checkpoint)\n",
        "\n",
        "  #Check acc\n",
        "\n",
        "  #check_accuracy(val_loader, model, device=DEVICE)\n",
        "\n",
        "  save_predictions_as_imgs(val_loader, model, folder=\"saved_images\", device=DEVICE)\n",
        "\n",
        "#Loader\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\Steven\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:116: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]C:\\Users\\Steven\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\cuda\\amp\\autocast_mode.py:118: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
            "  0%|          | 0/15 [00:55<?, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# CLASSIFIER"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ClassifierUnet(nn.Module):\n",
        "  #features represent the original paper dimensions.\n",
        "  def __init__(self, out_channels=1, linear=[1024, 250, 150, 100, 50], unetModel=None):\n",
        "    super(ClassifierUnet, self).__init__()\n",
        "\n",
        "    self.unetModel = unetModel\n",
        "    self.linear_layers = nn.Sequential()\n",
        "    #Create a list of linear classificator\n",
        "    linear_count = 1\n",
        "    activation_count = 1\n",
        "    self.linear_layers.add_module(str(linear_count)+\"_linear\", \n",
        "                                  nn.Linear(self.unetModel.linearFeature,\n",
        "                                            linear[0]))\n",
        "    self.linear_layers.add_module(str(activation_count)+\"_activ\", nn.ReLU())\n",
        "\n",
        "    linear_count+=1\n",
        "    activation_count+=1\n",
        "\n",
        "    actual_linear = linear[0]\n",
        "\n",
        "    for linear_dimesion in linear[0:]:\n",
        "      self.linear_layers.add_module(str(linear_count)+\"_linear\", nn.Linear(actual_linear, linear_dimesion))\n",
        "      self.linear_layers.add_module(str(activation_count)+\"_activ\", nn.ReLU())\n",
        "\n",
        "      linear_count+=1\n",
        "      activation_count+=1\n",
        "      \n",
        "      actual_linear = linear_dimesion\n",
        "    \n",
        "    self.linear_layers.add_module(str(linear_count)+\"_linear\", nn.Linear(actual_linear, out_channels))\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.unetModel(x)\n",
        "      return self.linear_layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ClassifierDataset(Dataset):\n",
        "  def __init__(self, image_dir, class_dic, transform=None):\n",
        "    self.image_dir = image_dir\n",
        "    self.transform = transform\n",
        "    self.images = []\n",
        "    self.can_classes = len(class_dic)\n",
        "    self.class_dic = class_dic\n",
        "    \n",
        "    for subdir, dirs, files in os.walk(self.image_dir):\n",
        "      for filename in files:\n",
        "        subdirectoryPath = os.path.relpath(subdir, self.image_dir) #get the path to your subdirectory\n",
        "        filePath = os.path.join(self.image_dir, os.path.join(subdirectoryPath, filename)) #get the path to your file\n",
        "        self.images.append([filePath, subdirectoryPath])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "     img_path = self.images[index][0]\n",
        "    #  print( self.images[index])\n",
        "    #  print( self.can_classes)\n",
        "\n",
        "     #The image input is an RBG but the image mask is in a grayscale\n",
        "     image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "     label = np.array(self.class_dic[self.images[index][1]])\n",
        "     \n",
        "     #mask[mask == 255.0] = 1.0\n",
        "     if self.transform is not None:\n",
        "       augmentations = self.transform(image=image)\n",
        "       image = augmentations[\"image\"]\n",
        "     return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 3])\ntorch.Size([5, 3, 160, 160])\n"
          ]
        }
      ],
      "source": [
        "def test():\n",
        "  #Batch, Channel, H, W\n",
        "  x = torch.randn((5,3,160,160))\n",
        "  unetTest = UNET(in_channels=3,out_channels=3, isOnlyEncoder=True)\n",
        "  classifierUNET = ClassifierUnet(out_channels=3, unetModel=unetTest)\n",
        "  preds = classifierUNET(x)\n",
        "  print(preds.shape)\n",
        "  print(x.shape)\n",
        "\n",
        "  #assert preds.shape == x.shape\n",
        "\n",
        "test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_loaders(\n",
        "    train_dir,\n",
        "    val_dir,\n",
        "    batch_size,\n",
        "    train_transform,\n",
        "    val_transform,\n",
        "    class_dic,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "):\n",
        "  train_ds = ClassifierDataset(\n",
        "      image_dir = train_dir,\n",
        "      class_dic  = class_dic,\n",
        "      transform = train_transform, \n",
        "  )\n",
        "  train_loader = DataLoader(\n",
        "      train_ds,\n",
        "      batch_size=batch_size,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=pin_memory,\n",
        "      shuffle=True,\n",
        "  )\n",
        "\n",
        "  val_ds = ClassifierDataset(\n",
        "      image_dir = val_dir,\n",
        "      class_dic  = class_dic,\n",
        "      transform = val_transform, \n",
        "  )\n",
        "  val_loader = DataLoader(\n",
        "      val_ds,\n",
        "      batch_size=batch_size,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=pin_memory,\n",
        "      shuffle=False,\n",
        "  )\n",
        "\n",
        "  return train_loader, val_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_CLASS_IMG_DIR = \"G:/Mi unidad/plants/classifier_train\"\n",
        "VAL_CLASS_IMG_DIR = \"G:/Mi unidad/plants/classifier_val\"\n",
        "\n",
        "CLASS_DIC = {\n",
        "    \"Apple___Apple_scab\":0,\n",
        "    \"Apple___Black_rot\":1,\n",
        "    \"Apple___Cedar_apple_rust\":2\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def train_fn2(loader, model, optimizer, loss_fn, scaler):\n",
        "  #Progress bar\n",
        "  loop = tqdm(loader)\n",
        "\n",
        "  for batch_idx, (data, targets) in enumerate (loop):\n",
        "    data = data.to(device=DEVICE)\n",
        "    targets = targets.long().to(device=DEVICE)\n",
        "    \n",
        "    #Forward\n",
        "    #Run with mixture precision float 32 ops with float16 and stuffs like that.\n",
        "    with torch.cuda.amp.autocast():\n",
        "      predictions = model(data)\n",
        "      predictions = predictions.float().to(device=DEVICE)\n",
        "      loss = loss_fn(predictions, targets)\n",
        "      \n",
        "    #Backwards\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer=optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    #update tqdm loop\n",
        "\n",
        "    loop.set_postfix(loss=loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_transform = A.Compose([\n",
        "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "    A.Rotate(limit=35, p=1.0),\n",
        "    A.HorizontalFlip(p=0.1),\n",
        "    A.Normalize(\n",
        "        mean=[0.0, 0.0, 0.0],\n",
        "        std=[1.0, 1.0, 1.0],\n",
        "        max_pixel_value=255.0,\n",
        "    ),\n",
        "    ToTensorV2(),\n",
        "    #TODO change if is a gray scale image\n",
        "],additional_targets={'image': 'image'})\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "    A.Normalize(\n",
        "        mean=[0.0, 0.0, 0.0],\n",
        "        std=[1.0, 1.0, 1.0],\n",
        "        max_pixel_value=255.0,\n",
        "    ),\n",
        "    ToTensorV2(),\n",
        "    #TODO\n",
        "],additional_targets={'image': 'image'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "NUM_CLASSES = 3\n",
        "unetTest = UNET(in_channels=3,out_channels=3, isOnlyEncoder=True)\n",
        "unetTest.eval()\n",
        "if LOAD_MODEL:\n",
        "        load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), unetTest)\n",
        "classifierUNET = ClassifierUnet(out_channels=3, unetModel=unetTest)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "train_loader, val_loader = get_loaders(\n",
        "    TRAIN_CLASS_IMG_DIR,\n",
        "    VAL_CLASS_IMG_DIR,\n",
        "    BATCH_SIZE,\n",
        "    train_transform,\n",
        "    val_transform,\n",
        "    CLASS_DIC,\n",
        "    NUM_WORKERS,\n",
        "    PIN_MEMORY,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\Steven\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:116: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
            "  0%|          | 0/61 [00:00<?, ?it/s]C:\\Users\\Steven\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\cuda\\amp\\autocast_mode.py:118: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
            " 89%|████████▊ | 54/61 [01:22<00:10,  1.52s/it, loss=1.11]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-137-3482364d3456>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m   \u001b[0mtrain_fn2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifierUNET\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   checkpoint = {\n",
            "\u001b[1;32m<ipython-input-134-f4af95b67f33>\u001b[0m in \u001b[0;36mtrain_fn2\u001b[1;34m(loader, model, optimizer, loss_fn, scaler)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m#Run with mixture precision float 32 ops with float16 and stuffs like that.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m       \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m       \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m       \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-122-e6155d4d3092>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munetModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-30-0dbdc952fce5>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mskip_connections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdown\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdowns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m       \u001b[0mskip_connections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-29-4ba1556947a3>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    393\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 395\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    396\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "scaler = torch.cuda.amp.GradScaler()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  train_fn2(train_loader, classifierUNET, optimizer, loss, scaler)\n",
        "\n",
        "  checkpoint = {\n",
        "      \"state_dict\"  : model.state_dict(),\n",
        "      \"optimizer\"   : optimizer.state_dict()\n",
        "  }\n",
        "\n",
        "  save_checkpoint(checkpoint)\n",
        "\n",
        "  #Check acc\n",
        "\n",
        "  #check_advance(val_loader, classifierUNET, device=DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}